{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "import random\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, RepeatVector, Masking\n",
    "from keras.layers import Convolution2D, Dense, Flatten, merge, MaxPooling2D, Input, AveragePooling2D, Lambda, Merge, Activation, Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.optimizers import SGD, Adam, rmsprop\n",
    "from keras import backend as K\n",
    "\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "import tensorflow as tf\n",
    "\n",
    "#LSTM will cause problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 3)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img,size)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drqn(input_shape, action_size, learning_rate):\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(TimeDistributed(tf.keras.layers.Conv2D(32, 8, 4, activation='relu'), \n",
    "                                  input_shape=(input_shape)))\n",
    "        model.add(TimeDistributed(tf.keras.layers.Conv2D(64, 4, 3, activation='relu')))\n",
    "        model.add(TimeDistributed(tf.keras.layers.Conv2D(64, 3, 3, activation='relu')))\n",
    "        model.add(TimeDistributed(tf.keras.layers.Flatten()))\n",
    "\n",
    "        # Use all traces for training\n",
    "        #model.add(LSTM(512, return_sequences=True,  activation='tanh'))\n",
    "        #model.add(TimeDistributed(Dense(output_dim=action_size, activation='linear')))\n",
    "\n",
    "        # Use last trace for training\n",
    "        model.add(tf.keras.layers.LSTM(512,  activation='tanh'))\n",
    "        model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Memory Replay Buffer \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size=10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, episode_experience):\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(episode_experience)\n",
    "\n",
    "    def sample(self, batch_size, trace_length):\n",
    "        sampled_episodes = random.sample(self.buffer, batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0, len(episode)+1-trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return sampledTraces\n",
    "\n",
    "class DoubleDQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, trace_length):\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.observe = 5000\n",
    "        self.explore = 50000\n",
    "        self.frame_per_action = 4\n",
    "        self.trace_length = trace_length\n",
    "        self.update_target_freq = 3000\n",
    "        self.timestep_per_train = 5 # Number of timesteps between training interval\n",
    "\n",
    "        # Create replay memory\n",
    "        self.memory = ReplayMemory()\n",
    "\n",
    "        # Create main model and target model\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_ammo_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        After some time interval update the target model to be same with model\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action from model using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_idx = random.randrange(self.action_size)\n",
    "        else:\n",
    "  \n",
    "            # Use all traces for RNN\n",
    "            #q = self.model.predict(state) # 1x8x3\n",
    "            #action_idx = np.argmax(q[0][-1])\n",
    "\n",
    "            # Only use last trace for RNN\n",
    "            q = self.model.predict(state) # 1x3\n",
    "            action_idx = np.argmax(q)\n",
    "        return action_idx\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 1\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use ammo\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        return r_t\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "\n",
    "        sample_traces = self.memory.sample(self.batch_size, self.trace_length) # 32x8x4\n",
    "\n",
    "        # Shape (batch_size, trace_length, img_rows, img_cols, color_channels)\n",
    "        update_input = np.zeros(((self.batch_size,) + self.state_size)) # 32x8x64x64x3\n",
    "        update_target = np.zeros(((self.batch_size,) + self.state_size))\n",
    "\n",
    "        action = np.zeros((self.batch_size, self.trace_length)) # 32x8\n",
    "        reward = np.zeros((self.batch_size, self.trace_length))\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.trace_length):\n",
    "                update_input[i,j,:,:,:] = sample_traces[i][j][0]\n",
    "                action[i,j] = sample_traces[i][j][1]\n",
    "                reward[i,j] = sample_traces[i][j][2]\n",
    "                update_target[i,j,:,:,:] = sample_traces[i][j][3]\n",
    "\n",
    "        \"\"\"\n",
    "        # Use all traces for training\n",
    "        # Size (batch_size, trace_length, action_size)\n",
    "        target = self.model.predict(update_input) # 32x8x3\n",
    "        target_val = self.model.predict(update_target) # 32x8x3\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.trace_length):\n",
    "                a = np.argmax(target_val[i][j])\n",
    "                target[i][j][int(action[i][j])] = reward[i][j] + self.gamma * (target_val[i][j][a])\n",
    "        \"\"\"\n",
    "\n",
    "        # Only use the last trace for training\n",
    "        target = self.model.predict(update_input) # 32x3\n",
    "        target_val = self.model.predict(update_target) # 32x3\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = np.argmax(target_val[i])\n",
    "            target[i][int(action[i][-1])] = reward[i][-1] + self.gamma * (target_val[i][a])\n",
    "\n",
    "        loss = self.model.train_on_batch(update_input, target)\n",
    "\n",
    "        return np.max(target[-1,-1]), loss\n",
    "\n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models\")\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #onfig.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "\n",
    "     with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/rocket_basic.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.set_window_visible(True)\n",
    "        game.set_mode(vzd.Mode.PLAYER)\n",
    "        game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "        game.get_available_buttons_size()\n",
    "        game.init()\n",
    "\n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "\n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 45\n",
    "        img_channels = 3 # Color channel\n",
    "        trace_length = 4 # Temporal Dimension\n",
    "\n",
    "        state_size = (trace_length, img_rows, img_cols, img_channels)\n",
    "        agent = DoubleDQNAgent(state_size, action_size, trace_length)\n",
    "\n",
    "        agent.model = drqn(state_size, action_size, agent.learning_rate)\n",
    "        agent.target_model = Networks.drqn(state_size, action_size, agent.learning_rate)\n",
    "\n",
    "        s_t = game_state.screen_buffer # 480 x 640\n",
    "        s_t = preprocessImg(s_t, size=(img_rows, img_cols))\n",
    "\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "    # Start training\n",
    "        epsilon = agent.initial_epsilon\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "        life = 0\n",
    "        episode_buf = [] # Save entire episode\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "\n",
    "        loss = 0\n",
    "        Q_max = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([action_size])\n",
    "        \n",
    "        # Epsilon Greedy\n",
    "        if len(episode_buf) > agent.trace_length:\n",
    "            # 1x8x64x64x3\n",
    "            state_series = np.array([trace[-1] for trace in episode_buf[-agent.trace_length:]])\n",
    "            state_series = np.expand_dims(state_series, axis=0)\n",
    "            action_idx  = agent.get_action(state_series)\n",
    "        else:\n",
    "            action_idx = random.randrange(agent.action_size)\n",
    "        a_t[action_idx] = 1\n",
    "\n",
    "        a_t = a_t.astype(int)\n",
    "        game.set_action(a_t.tolist())\n",
    "        skiprate = agent.frame_per_action\n",
    "        game.advance_action(skiprate)\n",
    "\n",
    "        game_state = game.get_state()  # Observe again after we take the action\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "        r_t = game.get_last_reward()  #each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "\n",
    "        if (is_terminated):\n",
    "            if (life > max_life):\n",
    "                max_life = life\n",
    "            GAME += 1\n",
    "            life_buffer.append(life)\n",
    "            ammo_buffer.append(misc[1])\n",
    "            kills_buffer.append(misc[0])\n",
    "            print (\"Episode Finish \", misc)\n",
    "            game.new_episode()\n",
    "            game_state = game.get_state()\n",
    "            misc = game_state.game_variables\n",
    "            s_t1 = game_state.screen_buffer\n",
    "\n",
    "            s_t1 = game_state.screen_buffer\n",
    "            misc = game_state.game_variables\n",
    "            s_t1 = preprocessImg(s_t1, size=(img_rows, img_cols))\n",
    "\n",
    "            r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            if (is_terminated):\n",
    "                life = 0\n",
    "            else:\n",
    "                life += 1\n",
    "\n",
    "        #update the cache\n",
    "            prev_misc = misc\n",
    "\n",
    "        # Update epsilon\n",
    "            if agent.epsilon > agent.final_epsilon and t > agent.observe:\n",
    "                agent.epsilon -= (agent.initial_epsilon - agent.final_epsilon) / agent.explore\n",
    "\n",
    "        # Do the training\n",
    "            if t > agent.observe:\n",
    "                Q_max, loss = agent.train_replay()\n",
    "\n",
    "        # save the sample <s, a, r, s'> to episode buffer\n",
    "            episode_buf.append([s_t, action_idx, r_t, s_t1])\n",
    "\n",
    "            if (is_terminated):\n",
    "                agent.memory.add(episode_buf)\n",
    "                episode_buf = [] # Reset Episode Buf\n",
    "\n",
    "            s_t = s_t1\n",
    "            t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                print(\"Now we save model\")\n",
    "                agent.model.save_weights(\"models/drqn.h5\", overwrite=True)\n",
    "\n",
    "        # print info\n",
    "            state = \"\"\n",
    "            if t <= agent.observe:\n",
    "                state = \"observe\"\n",
    "            elif t > agent.observe and t <= agent.observe + agent.explore:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "\n",
    "            if (is_terminated):\n",
    "                print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \\\n",
    "                      \"/ EPSILON\", agent.epsilon, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \\\n",
    "                      \"/ Q_MAX %e\" % np.max(Q_max), \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "            # Save Agent's Performance Statistics\n",
    "                if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                    print(\"Update Rolling Statistics\")\n",
    "                    agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                    agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                    agent.mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                    agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "\n",
    "                # Reset rolling stats buffer\n",
    "                    life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "                # Write Rolling Statistics to file\n",
    "                    with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/drqn_stats.txt\", \"w\") as stats_file:\n",
    "                        stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                        stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                        stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                        stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                        stats_file.write('mavg_ammo_left: ' + str(agent.mavg_ammo_left) + '\\n')\n",
    "                        stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
