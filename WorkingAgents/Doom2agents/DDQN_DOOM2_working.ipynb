{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/skimage/viewer/utils/__init__.py:1: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  from .core import *\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "#from skimage.viewer.core import *\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import backend as K\n",
    "import vizdoom as vzd\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(input_shape, action_size, learning_rate):\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(32, 8, 4, activation='relu', input_shape=(input_shape)))\n",
    "        model.add(tf.keras.layers.Conv2D(64, 4, 2, activation='relu'))\n",
    "        model.add(tf.keras.layers.Conv2D(64, 3, 1, activation='relu', padding='same'))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(action_size, activation='linear'))\n",
    "\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='MSE',optimizer=adam)\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 2)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img,size)\n",
    "    img = skimage.color.rgb2gray(img)\n",
    "\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.observe = 16#5000\n",
    "        self.explore = 32#50000 \n",
    "        self.frame_per_action = 4\n",
    "        self.update_target_freq = 8#3000 \n",
    "        self.timestep_per_train = 4 #100 # Number of timesteps between training interval\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque()\n",
    "        self.max_memory = 50000 # number of previous transitions to remember\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_pistol_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "        self.mavg_shotgun_left = []\n",
    "        self.mavg_minigun_left = []\n",
    "        self.mavg_plasma_left = []\n",
    "        self.mavg_rocket_left = []\n",
    "        self.mavg_secret_left = []\n",
    "        self.mavg_hit_count = []\n",
    "        self.mavg_damage_given = []\n",
    "        self.mavg_item_collected = []\n",
    "        self.mavg_armor = []\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        After some time interval update the target model to be same with model\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action from model using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_idx = random.randrange(self.action_size)\n",
    "        else:\n",
    "            q = self.model.predict(state)\n",
    "            action_idx = np.argmax(q)\n",
    "        return action_idx\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 310\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use pistol\n",
    "            r_t = r_t - 1\n",
    "        if (misc[1] > prev_misc[1]):\n",
    "            r_t = r_t + 1\n",
    "        \n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 2\n",
    "        if (misc[2] > prev_misc[2]):\n",
    "            r_t = r_t + 2.25\n",
    "        \n",
    "        if (misc[3] < prev_misc[3]): # Loss shotgun\n",
    "            r_t = r_t - 3\n",
    "        if (misc[3] < prev_misc[3]):\n",
    "            r_t = r_t +3\n",
    "        \n",
    "        if (misc[4] < prev_misc[4]): # Loss minigun\n",
    "            r_t = r_t - 1\n",
    "        if (misc[4] < prev_misc[4]):\n",
    "            r_t = r_t + 1\n",
    "        \n",
    "        if (misc[5] < prev_misc[5]): # plasma\n",
    "            r_t = r_t - 2\n",
    "        if (misc[5] < prev_misc[5]):\n",
    "            r_t = r_t + 2\n",
    "            \n",
    "        if (misc[6] < prev_misc[6]): # rocket\n",
    "            r_t = r_t - 5\n",
    "        if (misc[6] > prev_misc[6]):\n",
    "            r_t = r_t + 5\n",
    "            \n",
    "        if (misc[7] > prev_misc[7]): # secrets\n",
    "            r_t = r_t + 15 \n",
    "                    \n",
    "        if (misc[8] > prev_misc[8]): # hitcount\n",
    "            r_t = r_t + 5\n",
    "            \n",
    "        if (misc[9] > prev_misc[9]): # hits taken\n",
    "            r_t = r_t - 5\n",
    "        \n",
    "        if (misc[10] > prev_misc[10]): # items picked up\n",
    "            r_t = r_t + 2\n",
    "        \n",
    "        if (misc[11] < prev_misc[11]): # armor\n",
    "            r_t = r_t - 1\n",
    "        if (misc[11] > prev_misc[11]):\n",
    "            r_t = r_t + 1.5\n",
    "        \n",
    "\n",
    "        return r_t\n",
    "\n",
    "    # Save trajectory sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, s_t, action_idx, r_t, s_t1, is_terminated, t):\n",
    "        self.memory.append((s_t, action_idx, r_t, s_t1, is_terminated))\n",
    "        if self.epsilon > self.final_epsilon and t > self.observe:\n",
    "            self.epsilon -= (self.initial_epsilon - self.final_epsilon) / self.explore\n",
    "\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.popleft()\n",
    "\n",
    "        # Update the target model to be same with model\n",
    "        if t % self.update_target_freq == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_minibatch_replay(self):\n",
    "        \"\"\"\n",
    "        Train on a single minibatch\n",
    "        \"\"\"\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros(((batch_size,) + self.state_size)) # Shape 64, img_rows, img_cols, 4\n",
    "        update_target = np.zeros(((batch_size,) + self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i,:,:,:] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i,:,:,:] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) # Shape 64, Num_Actions\n",
    "\n",
    "        target_val = self.model.predict(update_target)\n",
    "        target_val_ = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_val[i])\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (target_val_[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        loss = self.model.train_on_batch(update_input, target)\n",
    "\n",
    "        return np.max(target[-1]), loss\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "\n",
    "        num_samples = min(self.batch_size * self.timestep_per_train, len(self.memory))\n",
    "        replay_samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "        update_input = np.zeros(((num_samples,) + self.state_size)) \n",
    "        update_target = np.zeros(((num_samples,) + self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            update_input[i,:,:,:] = replay_samples[i][0]\n",
    "            action.append(replay_samples[i][1])\n",
    "            reward.append(replay_samples[i][2])\n",
    "            update_target[i,:,:,:] = replay_samples[i][3]\n",
    "            done.append(replay_samples[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) \n",
    "        target_val = self.model.predict(update_target)\n",
    "        target_val_ = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_val[i])\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (target_val_[i][a])\n",
    "        callbacks = [tf.keras.callbacks.TensorBoard(log_dir=\"/home/spillingvoid/Downloads/programs/Doom/statistics/\", \n",
    "                                                    histogram_freq=1, write_graph=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=\"/home/spillingvoid/Downloads/programs/Doom/models/DDQN_weights\",\n",
    "                save_weights_only=True),\n",
    "                    ]\n",
    "        loss = self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=2, callbacks=callbacks)\n",
    "\n",
    "        return np.max(target[-1]), loss.history['loss']\n",
    "\n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/ddqn.h5\")\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/ddqn.h5\")\n",
    "        self.model.save_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/ddqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishing():\n",
    "    print(\"saving model\")\n",
    "    agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)\n",
    "    print(\"Save Complete\")\n",
    "    game.close()\n",
    "    print(\"======================================\")\n",
    "    print(\"Training is finished.\")\n",
    "    print(\"Training complete\")\n",
    "    endtime = time()\n",
    "    print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))\n",
    "    quit\n",
    "    sys.exit()\n",
    "    %reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 6, 21, 32)         8224      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 669,859\n",
      "Trainable params: 669,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 6, 21, 32)         8224      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 669,859\n",
      "Trainable params: 669,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-44a8d3d7f5a7>:5: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  img = skimage.color.rgb2gray(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - loss: 8.3163\n",
      "1/1 - 0s - loss: 7.7843\n",
      "1/1 - 0s - loss: 7.3519\n",
      "2/2 - 0s - loss: 7.1715\n",
      "2/2 - 0s - loss: 6.8039\n",
      "2/2 - 0s - loss: 7.3444\n",
      "2/2 - 0s - loss: 6.9270\n",
      "2/2 - 0s - loss: 7.4471\n",
      "2/2 - 0s - loss: 6.8899\n",
      "2/2 - 0s - loss: 7.2287\n",
      "2/2 - 0s - loss: 6.8262\n",
      "3/3 - 0s - loss: 7.3792\n",
      "3/3 - 0s - loss: 6.0353\n",
      "3/3 - 0s - loss: 6.6987\n",
      "Episode Finish  [  0.  41. 100.]\n",
      "TIME 75 / GAME 1 / STATE train / EPSILON 9.999999999978082e-05 / ACTION 1 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "3/3 - 0s - loss: 4.1408\n",
      "3/3 - 0s - loss: 4.6758\n",
      "3/3 - 0s - loss: 2.2621\n",
      "3/3 - 0s - loss: 3.6387\n",
      "3/3 - 0s - loss: 1.6164\n",
      "4/4 - 0s - loss: 4.3492\n",
      "4/4 - 0s - loss: 1.8893\n",
      "4/4 - 0s - loss: 3.7398\n",
      "4/4 - 0s - loss: 1.7455\n",
      "4/4 - 0s - loss: 5.1637\n",
      "4/4 - 0s - loss: 2.5094\n",
      "4/4 - 0s - loss: 5.6507\n",
      "4/4 - 0s - loss: 3.1531\n",
      "4/4 - 1s - loss: 5.6941\n",
      "4/4 - 0s - loss: 3.1259\n",
      "4/4 - 0s - loss: 5.9389\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 139 / GAME 2 / STATE train / EPSILON 9.999999999978082e-05 / ACTION 2 / REWARD 103.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "4/4 - 0s - loss: 47.2981\n",
      "4/4 - 0s - loss: 49.5363\n",
      "4/4 - 0s - loss: 48.6662\n",
      "4/4 - 0s - loss: 50.2038\n",
      "4/4 - 0s - loss: 50.1503\n",
      "4/4 - 0s - loss: 52.7894\n",
      "4/4 - 0s - loss: 5.2761\n",
      "4/4 - 0s - loss: 6.5216\n",
      "4/4 - 0s - loss: 4.7994\n",
      "4/4 - 0s - loss: 57.2183\n",
      "4/4 - 0s - loss: 55.1572\n",
      "4/4 - 0s - loss: 7.3204\n",
      "4/4 - 0s - loss: 54.7262\n",
      "4/4 - 0s - loss: 60.9520\n",
      "4/4 - 0s - loss: 60.1603\n",
      "4/4 - 0s - loss: 4.5099\n",
      "4/4 - 0s - loss: 61.8174\n",
      "4/4 - 0s - loss: 65.4692\n",
      "4/4 - 0s - loss: 7.8218\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 214 / GAME 3 / STATE train / EPSILON 9.999999999978082e-05 / ACTION 1 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "4/4 - 0s - loss: 65.2078\n",
      "4/4 - 0s - loss: 1.7837\n",
      "4/4 - 0s - loss: 72.6912\n",
      "4/4 - 0s - loss: 67.6320\n",
      "4/4 - 0s - loss: 63.8248\n",
      "4/4 - 0s - loss: 8.3125\n",
      "4/4 - 0s - loss: 72.4301\n",
      "4/4 - 0s - loss: 72.8155\n",
      "4/4 - 0s - loss: 66.7389\n",
      "4/4 - 0s - loss: 79.4257\n",
      "4/4 - 0s - loss: 10.9610\n",
      "4/4 - 0s - loss: 9.2060\n",
      "4/4 - 0s - loss: 70.3567\n",
      "4/4 - 0s - loss: 69.4781\n",
      "4/4 - 0s - loss: 78.4352\n",
      "4/4 - 0s - loss: 3.6052\n",
      "4/4 - 0s - loss: 80.6258\n",
      "4/4 - 0s - loss: 71.4427\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "4/4 - 0s - loss: 75.7913\n",
      "TIME 289 / GAME 4 / STATE train / EPSILON 9.999999999978082e-05 / ACTION 0 / REWARD -4.0 / Q_MAX -5.941453e+01 / LIFE 74 / LOSS [75.79133605957031]\n",
      "4/4 - 1s - loss: 13.7075\n",
      "4/4 - 0s - loss: 15.7796\n",
      "4/4 - 0s - loss: 23.2276\n",
      "4/4 - 0s - loss: 14.3436\n",
      "4/4 - 0s - loss: 89.2268\n",
      "4/4 - 0s - loss: 78.5637\n",
      "4/4 - 0s - loss: 16.8528\n",
      "4/4 - 0s - loss: 79.2031\n",
      "4/4 - 0s - loss: 27.1014\n",
      "4/4 - 0s - loss: 40.1885\n",
      "4/4 - 0s - loss: 83.0283\n",
      "4/4 - 0s - loss: 83.0602\n",
      "4/4 - 0s - loss: 17.3760\n",
      "4/4 - 0s - loss: 31.3759\n",
      "4/4 - 0s - loss: 100.4069\n",
      "4/4 - 0s - loss: 20.2972\n",
      "4/4 - 0s - loss: 115.5150\n",
      "4/4 - 0s - loss: 89.1016\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 364 / GAME 5 / STATE train / EPSILON 9.999999999978082e-05 / ACTION 1 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "saving model\n",
      "Save Complete\n",
      "======================================\n",
      "Training is finished.\n",
      "Training complete\n",
      " Test Time elapsed: 3.11 minutes\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "\n",
    "    with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/Doom/scenarios/Doom32.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO3)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO4)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO5)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO6)\n",
    "        game.add_available_game_variable(vzd.GameVariable.SECRETCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HITCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HITS_TAKEN)\n",
    "        game.add_available_game_variable(vzd.GameVariable.ITEMCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.ARMOR)\n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "\n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 90\n",
    "    # Convert image into Black and white\n",
    "        img_channels = 4 # We stack 4 frames\n",
    "\n",
    "        state_size = (img_rows, img_cols, img_channels)\n",
    "        agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "        agent.model = dqn(state_size, action_size, agent.learning_rate)\n",
    "        agent.target_model = dqn(state_size, action_size, agent.learning_rate)\n",
    "\n",
    "        x_t = game_state.screen_buffer # 480 x 640\n",
    "        x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "        s_t = np.stack(([x_t]*4), axis=2) # It becomes 64x64x4\n",
    "        s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "    # Start training\n",
    "        maxGAME = 4\n",
    "        epsilon = agent.initial_epsilon\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "        life = 0\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, pistol_buffer, kills_buffer, shotgun_buffer, minigun_buffer, plasma_buffer, rocket_buffer, secret_buffer, hit_buffer, damtaken_buffer, item_buffer, armor_buffer = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "            if GAME > maxGAME:\n",
    "                finishing()\n",
    "            loss = 0\n",
    "            Q_max = 0\n",
    "            r_t = 0\n",
    "            a_t = np.zeros([action_size])\n",
    "\n",
    "        # Epsilon Greedy\n",
    "            action_idx  = agent.get_action(s_t)\n",
    "            a_t[action_idx] = 1\n",
    "\n",
    "            a_t = a_t.astype(int)\n",
    "            game.set_action(a_t.tolist())\n",
    "            skiprate = agent.frame_per_action\n",
    "            game.advance_action(skiprate)\n",
    "\n",
    "            game_state = game.get_state()  # Observe again after we take the action\n",
    "            is_terminated = game.is_episode_finished()\n",
    "\n",
    "            r_t = game.get_last_reward()  #each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "\n",
    "            if (is_terminated):\n",
    "                if (life > max_life):\n",
    "                    max_life = life\n",
    "                GAME += 1\n",
    "                life_buffer.append(life)\n",
    "                pistol_buffer.append(misc[1])\n",
    "                kills_buffer.append(misc[2])\n",
    "                shotgun_buffer.append(misc[3])\n",
    "                minigun_buffer.append(misc[4])\n",
    "                plasma_buffer.append(misc[5])\n",
    "                rocket_buffer.append(misc[6])\n",
    "                secret_buffer.append(misc[7])\n",
    "                hit_buffer.append(misc[8])\n",
    "                damtaken_buffer.append(misc[9])\n",
    "                item_buffer.append(misc[10]) \n",
    "                armor_buffer.append(misc[11])\n",
    "                print (\"Episode Finish \", misc)\n",
    "                game.new_episode()\n",
    "                game_state = game.get_state()\n",
    "                misc = game_state.game_variables\n",
    "                x_t1 = game_state.screen_buffer\n",
    "\n",
    "            x_t1 = game_state.screen_buffer\n",
    "            misc = game_state.game_variables\n",
    "\n",
    "            x_t1 = preprocessImg(x_t1, size=(img_rows, img_cols))\n",
    "            x_t1 = np.reshape(x_t1, (1, img_rows, img_cols, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "            r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            if (is_terminated):\n",
    "                life = 0\n",
    "            else:\n",
    "                life += 1\n",
    "\n",
    "        # Update the cache\n",
    "            prev_misc = misc\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory and decrease epsilon\n",
    "            agent.replay_memory(s_t, action_idx, r_t, s_t1, is_terminated, t)\n",
    "\n",
    "        # Do the training\n",
    "            if t > agent.observe and t % agent.timestep_per_train == 0:\n",
    "                Q_max, loss = agent.train_replay()\n",
    "            \n",
    "            s_t = s_t1\n",
    "            t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                print(\"Now we save model\")\n",
    "                agent.model.save_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/ddqn.h5\", overwrite=True)\n",
    "\n",
    "        # print info\n",
    "            state = \"\"\n",
    "            if t <= agent.observe:\n",
    "                state = \"observe\"\n",
    "            elif t > agent.observe and t <= agent.observe + agent.explore:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "\n",
    "            if (is_terminated):\n",
    "                print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \\\n",
    "                      \"/ EPSILON\", agent.epsilon, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \\\n",
    "                      \"/ Q_MAX %e\" % np.max(Q_max), \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "            # Save Agent's Performance Statistics\n",
    "                if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                    print(\"Update Rolling Statistics\")\n",
    "                    agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                    agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                    agent.mavg_pistol_left.append(np.mean(np.array(pistol_buffer)))\n",
    "                    agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "                    agent.mavg_shotgun_left.append(np.mean(np.array(shotgun_buffer)))\n",
    "                    agent.mavg_minigun_left.append(np.mean(np.array(minigun_buffer)))\n",
    "                    agent.mavg_plasma_left.append(np.mean(np.array(plasma_buffer)))\n",
    "                    agent.mavg_rocket_left.append(np.mean(np.array(rocket_buffer)))\n",
    "                    agent.mavg_secret_left.append(np.mean(np.array(secret_buffer)))\n",
    "                    agent.mavg_hit_count.append(np.mean(np.array(hit_buffer)))\n",
    "                    agent.mavg_damage_given.append(np.mean(np.array(damtaken_buffer)))\n",
    "                    agent.mavg_item_collected.append(np.mean(np.array(item_buffer)))\n",
    "                    agent.mavg_armor.append(np.mean(np.array(armor_buffer)))\n",
    "                # Reset rolling stats buffer\n",
    "                    life_buffer, pistol_buffer, kills_buffer, shotgun_buffer, minigun_buffer, plasma_buffer, rocket_buffer, secret_buffer, hit_buffer, damtaken_buffer, item_buffer, armor_buffer = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "                # Write Rolling Statistics to file\n",
    "                    with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/ddqn_stats.txt\", \"a+\") as stats_file:\n",
    "                        stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                        stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                        stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                        stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                        stats_file.write('mavg_pistol_left: ' + str(agent.mavg_pistol_left) + '\\n')\n",
    "                        stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')\n",
    "                        stats_file.write('mavg_shotgun_left: ' + str(agent.mavg_shotgun_left) + '\\n')\n",
    "                        stats_file.write('mavg_minigun_left: ' + str(agent.mavg_minigun_left) + '\\n')\n",
    "                        stats_file.write('mavg_plasma_left: ' + str(agent.mavg_plasma_left) + '\\n')\n",
    "                        stats_file.write('mavg_rocket_left: ' + str(agent.mavg_rocket_left) + '\\n')\n",
    "                        stats_file.write('mavg_secret_left: ' + str(agent.mavg_secret_left) + '\\n')\n",
    "                        stats_file.write('mavg_hit_count: ' + str(agent.mavg_hit_count) + '\\n')\n",
    "                        stats_file.write('mavg_damage_given: ' + str(agent.mavg_damage_given) + '\\n')\n",
    "                        stats_file.write('mavg_item_collected: ' + str(agent.mavg_item_collected) + '\\n')\n",
    "                        stats_file.write('mavg_armor: ' + str(agent.mavg_armor) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
