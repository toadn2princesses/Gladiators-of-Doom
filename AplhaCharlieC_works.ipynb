{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-import operator\n",
    "\n",
    "# M. Kempka, T.Sternal, M.Wydmuch, Z.Boztoprak\n",
    "# Januar 2021\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import vizdoom as vzd\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.color, skimage.transform\n",
    "\n",
    "from tqdm import trange\n",
    "from random import sample\n",
    "from time import time, sleep\n",
    "from collections import deque\n",
    "#import tensorflow_datasets as tfds\n",
    "#import resnet\n",
    "#os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"]=\"2\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "tf.compat.v1.enable_eager_execution() #tf.compat.v1.disable_eager_execution() \n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client.device_lib import list_local_devices\n",
    "devices = list_local_devices()\n",
    "    #devices\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicas = mirrored_strategy.num_replicas_in_sync\n",
    "print(\"Num_replicas:\", num_replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning settings\n",
    "learning_rate = 0.00000025\n",
    "discount_factor = 0.99\n",
    "replay_memory_size = 1000000\n",
    "num_train_epochs = 2 #256      epochs 2 learning steps 32 \n",
    "learning_steps_per_epoch = 32 #2048\n",
    "target_net_update_steps =  4 #1024\n",
    "\n",
    "# NN learning settings\n",
    "#BUFFER = len(extractDigits(*argv))\n",
    "batch_size = 4\n",
    "BATCH_SIZE_PER_REPLICA = 2\n",
    "GLOBALBATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\n",
    "EPOCHS = 8\n",
    "epoch = 8\n",
    "#x = [0]\n",
    "K = keras.backend\n",
    "# Training regime \n",
    "test_episodes_per_epoch = 4\n",
    "\n",
    "# Other parameters\n",
    "frames_per_action = 3\n",
    "resolution = (45, 30)\n",
    "shape = (1, 30, 45, 2)\n",
    "episodes_to_watch = 2\n",
    "num_action = 3\n",
    "num_actions = 2 ** num_action\n",
    "\n",
    "save_model = True\n",
    "load = False\n",
    "skip_learning = False\n",
    "watch = True\n",
    "#dist_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration file path\n",
    "# Configuration file path\n",
    "config_file_path = \"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/rocket_basic.cfg\"\n",
    "model_savefolder = \"/home/spillingvoid/Downloads/programs/ac/model\"\n",
    "model_weight = \"/home/spillingvoid/Downloads/programs/ac/weights\"\n",
    "model_callback = \"/home/spillingvoid/Downloads/programs/ac/callback\"\n",
    "model_statistics = \"/home/spillingvoid/Downloads/programs/ac/statistics\"\n",
    "videos = \"/home/spillingvoid/Downloads/programs/ac/statistics\"\n",
    "plots = \"/home/spillingvoid/Downloads/programs/ac/statistics\"\n",
    "checkpoint_dir = '/trainingcheckpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\"\n",
    "    \n",
    "    \n",
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    \n",
    "    img = cv2.resize(img, resolution)\n",
    "    img = cv2.bitwise_not(img)\n",
    "    image = cv2.resize(img, (320,220))\n",
    "    cv2.imshow('img', image)\n",
    "    cv2.waitKey(250)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    img /= 255\n",
    "    print(\"Image dimensions:\", img.shape)\n",
    "       \n",
    "    return tf.stack(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_game():\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.BGR24)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.get_available_buttons_size()\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    return game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, num_actions=num_actions, epsilon=1, epsilon_min=0.1, epsilon_decay=0.9999, load=load):\n",
    "        #with mirrored_strategy.scope():\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = SGD(learning_rate)\n",
    "        print(\"Loading model from: \", model_savefolder) \n",
    "        self.dqn = tf.keras.models.load_model(model_savefolder)\n",
    "        if load:\n",
    "            print(\"Loading model from: \", model_savefolder) \n",
    "            self.dqn = tf.keras.models.load_model(model_savefolder)\n",
    "        else:\n",
    "            self.dqn = DQN(self.num_actions)\n",
    "            self.target_net = DQN(self.num_actions)\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.set_weights(self.dqn.get_weights())\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        if self.epsilon < np.random.uniform(0,1):\n",
    "            action = int(tf.argmax(self.dqn(tf.reshape(state, (1,30,45,1))), axis=1))\n",
    "        else:\n",
    "            action = np.random.choice(range(self.num_actions), 1)[0]\n",
    "\n",
    "        return action\n",
    "        #with mirrored_strategy.scope():\n",
    "    def train_dqn(self, samples):\n",
    "        screen_buf, actions, rewards, next_screen_buf, dones = split_tuple(samples)\n",
    "\n",
    "        row_ids = list(range(screen_buf.shape[0]))\n",
    "        ids = extractDigits(row_ids, actions)\n",
    "        done_ids = extractDigits(np.where(dones)[0])\n",
    "        \n",
    "            \n",
    "        with tf.GradientTape(persistent=False) as tape:\n",
    "            \n",
    "            tape.watch(self.dqn.trainable_variables)\n",
    "\n",
    "            Q_prev = tf.gather_nd(self.dqn(screen_buf), ids)\n",
    "            Q_next = self.target_net(next_screen_buf)\n",
    "            Q_next = tf.gather_nd(Q_next, extractDigits(row_ids, #best_next_action\n",
    "                            tf.argmax(agent.dqn(next_screen_buf), axis=1)))\n",
    "            q_target = rewards + self.discount_factor * Q_next #next best q values\n",
    "            \n",
    "            if len(done_ids)>0:\n",
    "                done_rewards = tf.gather_nd(rewards, done_ids)\n",
    "                q_target = tf.tensor_scatter_nd_update(tensor=q_target, \n",
    "                                indices=done_ids, updates=done_rewards) #target q values #2\n",
    "                \n",
    "            #DQN.Qvalues = tf.reducesum((all_Q_values * mask, axis=1, keepdims=True)\n",
    "                \n",
    "            \n",
    "            td_error = tf.keras.losses.MSE(q_target, Q_prev)\n",
    "                                                    \n",
    "            gradients = tape.gradient(td_error, self.dqn.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.dqn.trainable_variables),\n",
    "                                       experimental_aggregate_gradients=True)\n",
    "            return td_error\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "                    self.epsilon = self.epsilon_min\n",
    "    #@tf.function\n",
    "    def distributed_train_step(self, samples):\n",
    "            per_replica_losses = mirrored_strategy.run(self.train_dqn, args=(samples,)) #mirrored_strategy.run\n",
    "            if epoch % 2 == 0:\n",
    "\n",
    "                return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM,\n",
    "                                per_replica_losses, axis=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tuple(samples):\n",
    "    samples = np.array(samples, dtype=object)\n",
    "    screen_buf = tf.stack(samples[:,0])\n",
    "    actions = samples[:,1]\n",
    "    rewards = tf.stack(samples[:,2])\n",
    "    next_screen_buf = tf.stack(samples[:,3])\n",
    "    dones = tf.stack(samples[:,4])  \n",
    "    return screen_buf, actions, rewards, next_screen_buf, dones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractDigits(*argv):\n",
    "    #with mirrored_strategy.scope():\n",
    "    if len(argv)==1:\n",
    "        return list(map(lambda x: [x], argv[0]))\n",
    "\n",
    "    return list(map(lambda x,y: [x,y], argv[0], argv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_samples(memory):\n",
    "    #with mirrored_strategy.scope():\n",
    "    if len(memory) < batch_size:\n",
    "        sample_size = len(memory)\n",
    "    else:\n",
    "        sample_size = batch_size\n",
    "\n",
    "    return sample(memory, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, game, replay_memory):\n",
    "    time_start = time()\n",
    "\n",
    "    for episode in range(num_train_epochs):\n",
    "        train_scores = []\n",
    "        print(\"\\nEpoch %d\\n-------\" % (episode + 1))\n",
    "\n",
    "        game.new_episode()\n",
    "\n",
    "        for i in trange(learning_steps_per_epoch, leave=False):\n",
    "            state = game.get_state()\n",
    "            screen_buf = preprocess(state.screen_buffer)\n",
    "            action = agent.choose_action(screen_buf)\n",
    "            reward = game.make_action(actions[action], frames_per_action)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                next_screen_buf = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                next_screen_buf = tf.zeros(shape=screen_buf.shape)\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "\n",
    "                game.new_episode()\n",
    "\n",
    "            replay_memory.append((screen_buf, action, reward, next_screen_buf, done))\n",
    "\n",
    "            if i >= batch_size:\n",
    "                agent.train_dqn(get_samples(replay_memory)) #agent.distributed_train_step(get_samples(replay_memory)) \n",
    "       \n",
    "            if ((i % target_net_update_steps) == 0):\n",
    "                agent.update_target_net()\n",
    "\n",
    "        train_scores = np.array(train_scores)\n",
    "        print(\"Results: mean: %.1f±%.1f,\" % (train_scores.mean(), train_scores.std())), \\\n",
    "        #          \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())\n",
    "\n",
    "        test(test_episodes_per_epoch, game, agent)\n",
    "        print(\"Total elapsed time: %.2f minutes\" % ((time() - time_start) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_episodes_per_epoch, game, agent):\n",
    "    test_scores = []\n",
    "    #with mirrored_strategy.scope():\n",
    "    print(\"\\nTesting...\")\n",
    "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.choose_action(state)\n",
    "            game.make_action(actions[best_action_index], frames_per_action)\n",
    "\n",
    "        r = game.get_total_reward()\n",
    "        test_scores.append(r)\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    print(\"Results: mean: %.1f±%.1f,\" % (\n",
    "            test_scores.mean(), test_scores.std())) #, \"min: %.1f\" % test_scores.min(),\n",
    "        #      \"max: %.1f\" % test_scores.max())\n",
    "    print(\"reward\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv3D(1024, kernel_size=10, strides=4, input_shape=(1, 30, 45), activation =\"elu\", padding=\"same\")\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv3D(264, kernel_size=8, strides=3, activation =\"elu\", padding=\"same\")\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.4)\n",
    "        self.conv5 = tf.keras.layers.Conv3D(2048, kernel_size=3, strides=2, activation =\"elu\", padding=\"same\") #input_shape=(9, 14, 8)\n",
    "        self.drop2 = tf.keras.layers.Dropout(0.5)\n",
    "        self.conv2 = tf.keras.layers.Conv3D(512, kernel_size=2, strides=1, activation =\"elu\", padding=\"same\")\n",
    "        \n",
    "        self.elu   = tf.keras.layers.ELU()\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.hidden1 = tf.keras.layers.Dense(256, activation=\"elu\")\n",
    "        self.state_value = tf.keras.layers.Dense(1, activation=\"softmax\")\n",
    "        self.advantage = tf.keras.layers.Dense(num_actions)\n",
    "    \n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def call(self, x, training=True):\n",
    "        \n",
    "        #x = input_states(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        #x = self.depth(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.drop1(x)\n",
    "        #x = self.conv4(x)\n",
    "        #x = self.norm2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.elu(x)\n",
    "        #x = self.norm3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden1(x)\n",
    "        \n",
    "        x1 = x[:, :96]\n",
    "        x2 = x[:, 96:]\n",
    "        x1 = self.state_value(x1)\n",
    "        x2 = self.advantage(x2)\n",
    "        \n",
    "        x = x1 + (x2 - tf.reshape(tf.math.reduce_mean(x2, axis=1), shape=(-1,1))) #Q_values\n",
    "        return x\n",
    "    \n",
    "        model = DQN(num_actions)\n",
    "        keras.utils.plot_model(model, 'Doomddddqn.png', show_shapes=True)\n",
    "        model.save(model_savefolder)\n",
    "        model.save_weights(model_weight)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#distributed stuff attempt asynconously\n",
    "class UnaryUnchangedStructureDataset(UnaryDataset):\n",
    "    def__init__(self, input_dataset, variant_tensor):\n",
    "        self._input_dataset = input_dataset\n",
    "        super(UnaryUnchangedStructureDataset, self).__init__(\n",
    "            input_dataset, variant_tensor)\n",
    "        \n",
    "    @property\n",
    "    def element_spec(self):\n",
    "        rturn self._input_dataset.element_spec\n",
    "        \n",
    "class PrefetrchDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #with mirrored_strategy.scope():\n",
    "    maxlen=replay_memory_size\n",
    "    agent = DQNAgent()\n",
    "    game = initialize_game()\n",
    "    replay_memory = deque(maxlen=replay_memory_size)\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "    \n",
    "    with tf.device(DEVICE):\n",
    "    #with mirrored_strategy.scope():\n",
    "    #with strategy:\n",
    "        if not skip_learning:\n",
    "            print(\"Starting the training!\")\n",
    "\n",
    "            run(agent, game, replay_memory)\n",
    "\n",
    "            game.close()\n",
    "            print(\"======================================\")\n",
    "            print(\"Training is finished.\")\n",
    "\n",
    "            if save_model:\n",
    "                agent.dqn.save(model_savefolder)\n",
    "                testend = time()\n",
    "                print(\" Test Time elapsed: %.2f minutes\" % ((testend - teststart) / 60.0))\n",
    "            game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if watch:\n",
    "            game.set_window_visible(True)\n",
    "            game.set_mode(vzd.Mode.ASYNC_PLAYER)\n",
    "            game.init()\n",
    "\n",
    "            for _ in range(episodes_to_watch):\n",
    "                game.new_episode()\n",
    "                while not game.is_episode_finished():\n",
    "                    state = preprocess(game.get_state().screen_buffer)\n",
    "                    best_action_index = agent.choose_action(state)\n",
    "\n",
    "                    # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "                    game.set_action(actions[best_action_index])\n",
    "                    for _ in range(frames_per_action):\n",
    "                        game.advance_action()\n",
    "\n",
    "                # Sleep between episodes\n",
    "                sleep(1.0)\n",
    "                score = game.get_total_reward()\n",
    "                print(\"Total score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
