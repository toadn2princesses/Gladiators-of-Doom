{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "import random\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflowkeras.layers import Convolution2D, Dense, Flatten, MaxPooling2D, Input, AveragePooling2D, Lambda, Activation, Embedding\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras import backend as K\n",
    "from tensorflow import keras\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "import vizdoom as vzd\n",
    "\n",
    "from networks import Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 3)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img,size)\n",
    "    img = skimage.color.rgb2gray(img)\n",
    "\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.observe = 5000\n",
    "        self.explore = 50000 \n",
    "        self.frame_per_action = 4\n",
    "        self.update_target_freq = 3000 \n",
    "        self.timestep_per_train = 100 # Number of timesteps between training interval\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.max_memory = 50000 # number of previous transitions to remember\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_ammo_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        After some time interval update the target model to be same with model\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action from model using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_idx = random.randrange(self.action_size)\n",
    "        else:\n",
    "            q = self.model.predict(state)\n",
    "            action_idx = np.argmax(q)\n",
    "        return action_idx\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 1\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use ammo\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        return r_t\n",
    "\n",
    "    # Save trajectory sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, s_t, action_idx, r_t, s_t1, is_terminated, t):\n",
    "        self.memory.append((s_t, action_idx, r_t, s_t1, is_terminated))\n",
    "        if self.epsilon > self.final_epsilon and t > self.observe:\n",
    "            self.epsilon -= (self.initial_epsilon - self.final_epsilon) / self.explore\n",
    "\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.popleft()\n",
    "\n",
    "        # Update the target model to be same with model\n",
    "        if t % self.update_target_freq == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_minibatch_replay(self):\n",
    "        \"\"\"\n",
    "        Train on a single minibatch\n",
    "        \"\"\"\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros(((batch_size,) + self.state_size)) # Shape 64, img_rows, img_cols, 4\n",
    "        update_target = np.zeros(((batch_size,) + self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i,:,:,:] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i,:,:,:] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) # Shape 64, Num_Actions\n",
    "\n",
    "        target_val = self.model.predict(update_target)\n",
    "        target_val_ = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_val[i])\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (target_val_[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        loss = self.model.train_on_batch(update_input, target)\n",
    "\n",
    "        return np.max(target[-1]), loss\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "\n",
    "        num_samples = min(self.batch_size * self.timestep_per_train, len(self.memory))\n",
    "        replay_samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "        update_input = np.zeros(((num_samples,) + self.state_size)) \n",
    "        update_target = np.zeros(((num_samples,) + self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            update_input[i,:,:,:] = replay_samples[i][0]\n",
    "            action.append(replay_samples[i][1])\n",
    "            reward.append(replay_samples[i][2])\n",
    "            update_target[i,:,:,:] = replay_samples[i][3]\n",
    "            done.append(replay_samples[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) \n",
    "        target_val = self.model.predict(update_target)\n",
    "        target_val_ = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_val[i])\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (target_val_[i][a])\n",
    "\n",
    "        loss = self.model.fit(update_input, target, batch_size=self.batch_size, epoch=1, verbose=0)\n",
    "\n",
    "        return np.max(target[-1]), loss.history['loss']\n",
    "\n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    '''config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)'''\n",
    "\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(\"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/defend_the_center.cfg\")\n",
    "    game.set_sound_enabled(True)\n",
    "    game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "    game.set_window_visible(False)\n",
    "    game.init()\n",
    "\n",
    "    game.new_episode()\n",
    "    game_state = game.get_state()\n",
    "    misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "    prev_misc = misc\n",
    "\n",
    "    action_size = game.get_available_buttons_size()\n",
    "\n",
    "    img_rows , img_cols = 64, 64\n",
    "    # Convert image into Black and white\n",
    "    img_channels = 4 # We stack 4 frames\n",
    "\n",
    "    state_size = (img_rows, img_cols, img_channels)\n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "    agent.model = Networks.dueling_dqn(state_size, action_size, agent.learning_rate)\n",
    "    agent.target_model = Networks.dueling_dqn(state_size, action_size, agent.learning_rate)\n",
    "\n",
    "    x_t = game_state.screen_buffer # 480 x 640\n",
    "    x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "    s_t = np.stack(([x_t]*4), axis=2) # It becomes 64x64x4\n",
    "    s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "    is_terminated = game.is_episode_finished()\n",
    "\n",
    "    # Start training\n",
    "    epsilon = agent.initial_epsilon\n",
    "    GAME = 0\n",
    "    t = 0\n",
    "    max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "    life = 0\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "    life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "\n",
    "        loss = 0\n",
    "        Q_max = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([action_size])\n",
    "\n",
    "        # Epsilon Greedy\n",
    "        action_idx  = agent.get_action(s_t)\n",
    "        a_t[action_idx] = 1\n",
    "\n",
    "        a_t = a_t.astype(int)\n",
    "        game.set_action(a_t.tolist())\n",
    "        skiprate = agent.frame_per_action\n",
    "        game.advance_action(skiprate)\n",
    "\n",
    "        game_state = game.get_state()  # Observe again after we take the action\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "        r_t = game.get_last_reward()  #each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "\n",
    "        if (is_terminated):\n",
    "            if (life > max_life):\n",
    "                max_life = life\n",
    "            GAME += 1\n",
    "            life_buffer.append(life)\n",
    "            ammo_buffer.append(misc[1])\n",
    "            kills_buffer.append(misc[0])\n",
    "            print (\"Episode Finish \", misc)\n",
    "            game.new_episode()\n",
    "            game_state = game.get_state()\n",
    "            misc = game_state.game_variables\n",
    "            x_t1 = game_state.screen_buffer\n",
    "\n",
    "        x_t1 = game_state.screen_buffer\n",
    "        misc = game_state.game_variables\n",
    "\n",
    "        x_t1 = preprocessImg(x_t1, size=(img_rows, img_cols))\n",
    "        x_t1 = np.reshape(x_t1, (1, img_rows, img_cols, 1))\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "        if (is_terminated):\n",
    "            life = 0\n",
    "        else:\n",
    "            life += 1\n",
    "\n",
    "        # Update the cache\n",
    "        prev_misc = misc\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory and decrease epsilon\n",
    "        agent.replay_memory(s_t, action_idx, r_t, s_t1, is_terminated, t)\n",
    "\n",
    "        # Do the training\n",
    "        if t > agent.observe and t % agent.timestep_per_train == 0:\n",
    "            Q_max, loss = agent.train_replay()\n",
    "            \n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        if t % 10000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            agent.model.save_weights(\"models/dueling_ddqn.h5\", overwrite=True)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= agent.observe:\n",
    "            state = \"observe\"\n",
    "        elif t > agent.observe and t <= agent.observe + agent.explore:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        if (is_terminated):\n",
    "            print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \\\n",
    "                  \"/ EPSILON\", agent.epsilon, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \\\n",
    "                  \"/ Q_MAX %e\" % np.max(Q_max), \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "            # Save Agent's Performance Statistics\n",
    "            if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                print(\"Update Rolling Statistics\")\n",
    "                agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                agent.mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "\n",
    "                # Reset rolling stats buffer\n",
    "                life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "                # Write Rolling Statistics to file\n",
    "                with open(\"statistics/dueling_ddqn_stats.txt\", \"w\") as stats_file:\n",
    "                    stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                    stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                    stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                    stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                    stats_file.write('mavg_ammo_left: ' + str(agent.mavg_ammo_left) + '\\n')\n",
    "                    stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
