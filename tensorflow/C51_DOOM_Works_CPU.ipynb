{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/skimage/viewer/utils/__init__.py:1: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  from .core import *\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import math\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import Model\n",
    "from keras import backend as K\n",
    "import vizdoom as vzd\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 2)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img,size)\n",
    "    img = skimage.color.rgb2gray(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C51Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, num_atoms):\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.batch_size = 4 #32\n",
    "        self.observe = 32 #2000\n",
    "        self.explore = 64 # 50000\n",
    "        self.frame_per_action = 4\n",
    "        self.update_target_freq = 40 #3000 \n",
    "        self.timestep_per_train = 4 #100 # Number of timesteps between training interval\n",
    "\n",
    "        # Initialize Atoms\n",
    "        self.num_atoms = num_atoms # 51 for C51\n",
    "        self.v_max = 300 # Max possible score for Defend the center is 26 - 0.1*26 = 23.4\n",
    "        self.v_min = -600 # -0.1*26 - 1 = -3.6\n",
    "        self.delta_z = (self.v_max - self.v_min) / float(self.num_atoms - 1)\n",
    "        self.z = [self.v_min + i * self.delta_z for i in range(self.num_atoms)]\n",
    "\n",
    "        # Create replay memory using deque\n",
    "        self.memory = deque()\n",
    "        self.max_memory = 50000 # number of previous transitions to remember\n",
    "\n",
    "        # Models for value distribution\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_ammo_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        After some time interval update the target model to be same with model\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action from model using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_idx = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action_idx = self.get_optimal_action(state)\n",
    "\n",
    "        return action_idx\n",
    "\n",
    "    def get_optimal_action(self, state):\n",
    "        \"\"\"Get optimal action for a state\n",
    "        \"\"\"\n",
    "        z = self.model.predict(state) # Return a list [1x51, 1x51, 1x51]\n",
    "\n",
    "        z_concat = np.vstack(z)\n",
    "        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1) \n",
    "\n",
    "        # Pick action with the biggest Q value\n",
    "        action_idx = np.argmax(q)\n",
    "        \n",
    "        return action_idx\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 300\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use ammo\n",
    "            r_t = r_t - 10\n",
    "\n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        return r_t\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, s_t, action_idx, r_t, s_t1, is_terminated, t):\n",
    "        self.memory.append((s_t, action_idx, r_t, s_t1, is_terminated))\n",
    "        if self.epsilon > self.final_epsilon and t > self.observe:\n",
    "            self.epsilon -= (self.initial_epsilon - self.final_epsilon) / self.explore\n",
    "\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.popleft()\n",
    "\n",
    "        # Update the target model to be same with model\n",
    "        if t % self.update_target_freq == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "\n",
    "        num_samples = min(self.batch_size * self.timestep_per_train, len(self.memory))\n",
    "        replay_samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "        state_inputs = np.zeros(((num_samples,) + self.state_size)) \n",
    "        next_states = np.zeros(((num_samples,) + self.state_size)) \n",
    "        m_prob = [np.zeros((num_samples, self.num_atoms)) for i in range(action_size)]\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            state_inputs[i,:,:,:] = replay_samples[i][0]\n",
    "            action.append(replay_samples[i][1])\n",
    "            reward.append(replay_samples[i][2])\n",
    "            next_states[i,:,:,:] = replay_samples[i][3]\n",
    "            done.append(replay_samples[i][4])\n",
    "\n",
    "        z = self.model.predict(next_states) # Return a list [32x51, 32x51, 32x51]\n",
    "        z_ = self.model.predict(next_states) # Return a list [32x51, 32x51, 32x51]\n",
    "\n",
    "        # Get Optimal Actions for the next states (from distribution z)\n",
    "        optimal_action_idxs = []\n",
    "        z_concat = np.vstack(z)\n",
    "        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1) # length (num_atoms x num_actions)\n",
    "        q = q.reshape((num_samples, action_size), order='F')\n",
    "        optimal_action_idxs = np.argmax(q, axis=1)\n",
    "\n",
    "        # Project Next State Value Distribution (of optimal action) to Current State\n",
    "        for i in range(num_samples):\n",
    "            if done[i]: # Terminal State\n",
    "                # Distribution collapses to a single point\n",
    "                Tz = min(self.v_max, max(self.v_min, reward[i]))\n",
    "                bj = (Tz - self.v_min) / self.delta_z \n",
    "                m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                m_prob[action[i]][i][int(m_l)] += (m_u - bj)\n",
    "                m_prob[action[i]][i][int(m_u)] += (bj - m_l)\n",
    "            else:\n",
    "                for j in range(self.num_atoms):\n",
    "                    Tz = min(self.v_max, max(self.v_min, reward[i] + self.gamma * self.z[j]))\n",
    "                    bj = (Tz - self.v_min) / self.delta_z \n",
    "                    m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                    m_prob[action[i]][i][int(m_l)] += z_[optimal_action_idxs[i]][i][j] * (m_u - bj)\n",
    "                    m_prob[action[i]][i][int(m_u)] += z_[optimal_action_idxs[i]][i][j] * (bj - m_l)\n",
    "\n",
    "        loss = self.model.fit(state_inputs, m_prob, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "        return loss.history['loss']\n",
    "\n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/c51_ddqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_distribution_network(input_shape, num_atoms, action_size, learning_rate):\n",
    "        \"\"\"Model Value Distribution\n",
    "\n",
    "        With States as inputs and output Probability Distributions for all Actions\n",
    "        \"\"\"\n",
    "\n",
    "        state_input = tf.keras.Input(shape=(input_shape)) \n",
    "        cnn_feature = tf.keras.layers.Conv2D(32, 8, 4, activation='relu')(state_input)\n",
    "        cnn_feature = tf.keras.layers.Conv2D(64, 4, 2, activation='relu')(cnn_feature)\n",
    "        cnn_feature = tf.keras.layers.Conv2D(64, 3, 3, activation='relu', padding=\"same\")(cnn_feature)\n",
    "        cnn_feature = tf.keras.layers.Flatten()(cnn_feature)\n",
    "        cnn_feature = tf.keras.layers.Dense(512, activation='relu')(cnn_feature)\n",
    "\n",
    "        distribution_list = []\n",
    "        for i in range(action_size):\n",
    "            distribution_list.append(tf.keras.layers.Dense(num_atoms, activation='softmax')(cnn_feature))\n",
    "\n",
    "        model = Model(state_input, distribution_list)\n",
    "\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=adam)\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 45, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 6, 10, 32)    8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 4, 64)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 2, 64)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          66048       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 51)           26163       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 51)           26163       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 51)           26163       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 222,521\n",
      "Trainable params: 222,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30, 45, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 10, 32)    8224        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 4, 64)     32832       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 1, 2, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          66048       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 51)           26163       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 51)           26163       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 51)           26163       dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 222,521\n",
      "Trainable params: 222,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-97d2ae62978c>:5: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  img = skimage.color.rgb2gray(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 32 / GAME 1 / STATE observe / EPSILON 1.0 / ACTION 0 / REWARD 103.0 / LIFE 31 / LOSS 0\n",
      "Episode Finish  [  0.  38. 100.]\n",
      "TIME 107 / GAME 2 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 182 / GAME 3 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 251 / GAME 4 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 326 / GAME 5 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 401 / GAME 6 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [487.67236328125]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 476 / GAME 7 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 492 / GAME 8 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 567 / GAME 9 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 584 / GAME 10 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 659 / GAME 11 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 734 / GAME 12 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 809 / GAME 13 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [16544.880859375]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 843 / GAME 14 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 867 / GAME 15 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 942 / GAME 16 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 1017 / GAME 17 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [45684.35546875]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 1092 / GAME 18 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 1167 / GAME 19 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 1242 / GAME 20 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 1291 / GAME 21 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 1366 / GAME 22 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 1441 / GAME 23 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [142073.9375]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 1516 / GAME 24 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 1591 / GAME 25 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 1655 / GAME 26 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 1704 / GAME 27 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 1759 / GAME 28 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 1834 / GAME 29 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 1909 / GAME 30 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [195667.03125]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 1984 / GAME 31 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 2035 / GAME 32 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 2071 / GAME 33 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 2099 / GAME 34 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 2171 / GAME 35 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 2227 / GAME 36 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 2239 / GAME 37 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 2314 / GAME 38 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 2389 / GAME 39 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [1582387.875]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 2464 / GAME 40 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 2539 / GAME 41 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  50. 100.]\n",
      "TIME 2614 / GAME 42 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 2647 / GAME 43 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 2722 / GAME 44 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 2735 / GAME 45 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 2767 / GAME 46 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 2842 / GAME 47 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 2917 / GAME 48 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [2657740.5]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 2992 / GAME 49 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 3067 / GAME 50 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 3127 / GAME 51 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 3202 / GAME 52 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 3267 / GAME 53 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 3342 / GAME 54 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 3417 / GAME 55 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [4919000.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 3492 / GAME 56 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 3503 / GAME 57 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 3578 / GAME 58 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 3595 / GAME 59 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 3670 / GAME 60 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 3745 / GAME 61 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [7228509.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 3791 / GAME 62 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 3811 / GAME 63 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 3886 / GAME 64 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 3961 / GAME 65 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [2592762.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 4036 / GAME 66 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 4075 / GAME 67 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 4091 / GAME 68 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 4166 / GAME 69 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 4224 / GAME 70 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 4275 / GAME 71 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 4350 / GAME 72 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 4425 / GAME 73 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [15481312.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 4500 / GAME 74 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 4575 / GAME 75 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 4650 / GAME 76 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 4725 / GAME 77 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [4866994.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 4800 / GAME 78 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 4875 / GAME 79 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 4950 / GAME 80 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 4995 / GAME 81 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5007 / GAME 82 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 5082 / GAME 83 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5099 / GAME 84 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 5174 / GAME 85 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5191 / GAME 86 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5207 / GAME 87 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 5275 / GAME 88 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5287 / GAME 89 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 5327 / GAME 90 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5402 / GAME 91 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 5431 / GAME 92 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5439 / GAME 93 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 5514 / GAME 94 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 5589 / GAME 95 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [12158606.0]\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 5664 / GAME 96 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 5739 / GAME 97 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 5771 / GAME 98 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 5846 / GAME 99 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 5921 / GAME 100 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [16896098.0]\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 5996 / GAME 101 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 6071 / GAME 102 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 6146 / GAME 103 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 6221 / GAME 104 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [39450600.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 6296 / GAME 105 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 6371 / GAME 106 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 6446 / GAME 107 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 6455 / GAME 108 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 6519 / GAME 109 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 6594 / GAME 110 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 6669 / GAME 111 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [15468634.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 6744 / GAME 112 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 6819 / GAME 113 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 6894 / GAME 114 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 6969 / GAME 115 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [8589625.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 7011 / GAME 116 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 7086 / GAME 117 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 7123 / GAME 118 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 7198 / GAME 119 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 7252 / GAME 120 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 7295 / GAME 121 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 7370 / GAME 122 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 7391 / GAME 123 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 7447 / GAME 124 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 7467 / GAME 125 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 7542 / GAME 126 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 7563 / GAME 127 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 7595 / GAME 128 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 7670 / GAME 129 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 7699 / GAME 130 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 7774 / GAME 131 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 7827 / GAME 132 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 7902 / GAME 133 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 7977 / GAME 134 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [30797408.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 8052 / GAME 135 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 8127 / GAME 136 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 8202 / GAME 137 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 8277 / GAME 138 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [14464247.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 8335 / GAME 139 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 8410 / GAME 140 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 8485 / GAME 141 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [39486560.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 8560 / GAME 142 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 8635 / GAME 143 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 8710 / GAME 144 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 8785 / GAME 145 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [24348352.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 8860 / GAME 146 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 8935 / GAME 147 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 9010 / GAME 148 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 9085 / GAME 149 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [41869696.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 9107 / GAME 150 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 9119 / GAME 151 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 9127 / GAME 152 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 9202 / GAME 153 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 9277 / GAME 154 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [45206936.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 9319 / GAME 155 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 9383 / GAME 156 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 9407 / GAME 157 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 9482 / GAME 158 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 9557 / GAME 159 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [79667184.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 9583 / GAME 160 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 9623 / GAME 161 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 9698 / GAME 162 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 9773 / GAME 163 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [76568064.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 9848 / GAME 164 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 9867 / GAME 165 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 9899 / GAME 166 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 9947 / GAME 167 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Now we save model\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 10022 / GAME 168 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 10076 / GAME 169 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 10151 / GAME 170 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 10215 / GAME 171 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 10290 / GAME 172 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 10365 / GAME 173 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [242004960.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 10419 / GAME 174 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 10494 / GAME 175 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 10569 / GAME 176 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [105417104.0]\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 10635 / GAME 177 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 10647 / GAME 178 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 10679 / GAME 179 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 10754 / GAME 180 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 10829 / GAME 181 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [82307672.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 10904 / GAME 182 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 10923 / GAME 183 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 10998 / GAME 184 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 11027 / GAME 185 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 11102 / GAME 186 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 11139 / GAME 187 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 11214 / GAME 188 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 11289 / GAME 189 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [101119368.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 11364 / GAME 190 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 11439 / GAME 191 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 11514 / GAME 192 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 11589 / GAME 193 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [85856256.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 11623 / GAME 194 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 11698 / GAME 195 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 11773 / GAME 196 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [102469424.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 11831 / GAME 197 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 11906 / GAME 198 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  50. 100.]\n",
      "TIME 11981 / GAME 199 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [89231064.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 12039 / GAME 200 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 12114 / GAME 201 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 12189 / GAME 202 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [120124528.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 12264 / GAME 203 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 12303 / GAME 204 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 12378 / GAME 205 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 12387 / GAME 206 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 12462 / GAME 207 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 12537 / GAME 208 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [307681312.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 12612 / GAME 209 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 12687 / GAME 210 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 12762 / GAME 211 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 12837 / GAME 212 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [147775600.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 12912 / GAME 213 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 12935 / GAME 214 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 13010 / GAME 215 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 13085 / GAME 216 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [180169824.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 13160 / GAME 217 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 13207 / GAME 218 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 13282 / GAME 219 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 13357 / GAME 220 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS [382277216.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 13432 / GAME 221 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 13491 / GAME 222 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 13566 / GAME 223 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 13579 / GAME 224 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 13654 / GAME 225 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 13729 / GAME 226 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [94894688.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 13804 / GAME 227 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  41. 100.]\n",
      "TIME 13879 / GAME 228 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 13954 / GAME 229 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 14029 / GAME 230 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [959661440.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 14104 / GAME 231 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 14143 / GAME 232 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 14179 / GAME 233 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 14231 / GAME 234 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 14306 / GAME 235 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 14323 / GAME 236 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 14398 / GAME 237 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 14467 / GAME 238 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 14479 / GAME 239 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 14554 / GAME 240 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 14629 / GAME 241 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [178878208.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 14704 / GAME 242 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 14719 / GAME 243 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 14794 / GAME 244 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 14869 / GAME 245 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [78683440.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 14879 / GAME 246 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 14932 / GAME 247 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 14995 / GAME 248 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 15070 / GAME 249 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 15091 / GAME 250 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 15166 / GAME 251 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 15241 / GAME 252 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [175822816.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 15316 / GAME 253 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 15375 / GAME 254 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 15450 / GAME 255 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 15467 / GAME 256 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 15542 / GAME 257 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 15617 / GAME 258 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [416835904.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 15651 / GAME 259 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 15726 / GAME 260 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 15801 / GAME 261 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [172300224.0]\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 15876 / GAME 262 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 15927 / GAME 263 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 15979 / GAME 264 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 16054 / GAME 265 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 16129 / GAME 266 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [338905760.0]\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 16204 / GAME 267 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 16279 / GAME 268 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 16354 / GAME 269 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 16429 / GAME 270 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [257560064.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 16459 / GAME 271 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 16499 / GAME 272 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 16515 / GAME 273 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 16590 / GAME 274 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 16639 / GAME 275 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 16714 / GAME 276 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 16789 / GAME 277 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [193578688.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 16831 / GAME 278 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 16906 / GAME 279 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 16981 / GAME 280 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS [459427840.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 17039 / GAME 281 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 17114 / GAME 282 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 17189 / GAME 283 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [675617664.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 17207 / GAME 284 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 17282 / GAME 285 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 17357 / GAME 286 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [319143488.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 17432 / GAME 287 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 17507 / GAME 288 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 17582 / GAME 289 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 17657 / GAME 290 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [667672832.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 17732 / GAME 291 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 17807 / GAME 292 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 17882 / GAME 293 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 17957 / GAME 294 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [1096554496.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 18032 / GAME 295 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 18075 / GAME 296 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 18150 / GAME 297 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 18183 / GAME 298 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 18258 / GAME 299 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 18333 / GAME 300 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [369582720.0]\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 18408 / GAME 301 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 18483 / GAME 302 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 18495 / GAME 303 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 18570 / GAME 304 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 18645 / GAME 305 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [361169888.0]\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 18720 / GAME 306 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 18795 / GAME 307 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 18815 / GAME 308 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 18835 / GAME 309 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 18910 / GAME 310 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 18947 / GAME 311 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 18999 / GAME 312 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 19074 / GAME 313 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 19149 / GAME 314 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [865819712.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 19224 / GAME 315 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 19231 / GAME 316 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 19259 / GAME 317 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 19267 / GAME 318 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 19342 / GAME 319 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 19417 / GAME 320 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [2016320768.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 19455 / GAME 321 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 19530 / GAME 322 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 19605 / GAME 323 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [1055925568.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 19680 / GAME 324 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 19735 / GAME 325 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 19747 / GAME 326 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 19795 / GAME 327 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 19870 / GAME 328 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 19884 / GAME 329 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 19959 / GAME 330 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Now we save model\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 20034 / GAME 331 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 20099 / GAME 332 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 20131 / GAME 333 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 20175 / GAME 334 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 20250 / GAME 335 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 20325 / GAME 336 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [729618048.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 20400 / GAME 337 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 20407 / GAME 338 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 20482 / GAME 339 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 20557 / GAME 340 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [944055808.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 20632 / GAME 341 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -9.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 20707 / GAME 342 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 20782 / GAME 343 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 20799 / GAME 344 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 20874 / GAME 345 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 20939 / GAME 346 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 20951 / GAME 347 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 21026 / GAME 348 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 21101 / GAME 349 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [1399520512.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 21119 / GAME 350 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 21194 / GAME 351 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 21269 / GAME 352 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [675515072.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 21344 / GAME 353 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 21351 / GAME 354 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 21426 / GAME 355 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 21501 / GAME 356 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [242371456.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 21576 / GAME 357 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 21651 / GAME 358 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 21726 / GAME 359 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 21801 / GAME 360 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [1311354496.0]\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 21876 / GAME 361 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 21951 / GAME 362 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 22026 / GAME 363 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 22101 / GAME 364 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [1174266880.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 22111 / GAME 365 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 22186 / GAME 366 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 22261 / GAME 367 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [1093797632.0]\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 22332 / GAME 368 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  39. 100.]\n",
      "TIME 22407 / GAME 369 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 22482 / GAME 370 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 22491 / GAME 371 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 22507 / GAME 372 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 22582 / GAME 373 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 22657 / GAME 374 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [351168512.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 22667 / GAME 375 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 22703 / GAME 376 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 22711 / GAME 377 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 22786 / GAME 378 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 22861 / GAME 379 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [533613440.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 22936 / GAME 380 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 23011 / GAME 381 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 23086 / GAME 382 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 23161 / GAME 383 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [2219558400.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 23236 / GAME 384 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 23255 / GAME 385 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 23330 / GAME 386 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 23367 / GAME 387 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 23442 / GAME 388 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 23455 / GAME 389 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 23507 / GAME 390 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 23582 / GAME 391 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 23635 / GAME 392 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 23651 / GAME 393 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 23708 / GAME 394 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 23719 / GAME 395 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 23776 / GAME 396 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 23835 / GAME 397 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 23896 / GAME 398 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 23939 / GAME 399 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 24007 / GAME 400 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 24043 / GAME 401 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 24051 / GAME 402 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 24111 / GAME 403 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 24186 / GAME 404 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 24261 / GAME 405 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [1377535872.0]\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 24336 / GAME 406 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 24411 / GAME 407 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 24486 / GAME 408 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 24561 / GAME 409 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [324880480.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 24636 / GAME 410 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 24711 / GAME 411 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 24786 / GAME 412 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 24861 / GAME 413 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [1146689024.0]\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 24936 / GAME 414 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 25011 / GAME 415 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 25086 / GAME 416 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 25161 / GAME 417 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [2145958016.0]\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 25236 / GAME 418 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 25267 / GAME 419 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 25342 / GAME 420 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 25417 / GAME 421 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [472714048.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 25492 / GAME 422 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 25507 / GAME 423 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 25582 / GAME 424 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 25657 / GAME 425 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [2781265664.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 25680 / GAME 426 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 25687 / GAME 427 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 25762 / GAME 428 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 25837 / GAME 429 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [1047585536.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 25912 / GAME 430 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 25987 / GAME 431 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 26062 / GAME 432 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 26091 / GAME 433 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 26166 / GAME 434 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 26199 / GAME 435 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 26274 / GAME 436 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  50. 100.]\n",
      "TIME 26349 / GAME 437 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [1415584128.0]\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 26424 / GAME 438 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 26499 / GAME 439 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 26547 / GAME 440 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 26555 / GAME 441 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 26630 / GAME 442 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 26671 / GAME 443 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 26679 / GAME 444 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "TIME 26754 / GAME 445 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 26829 / GAME 446 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [1395656704.0]\n",
      "Episode Finish  [  0.  49. 100.]\n",
      "TIME 26855 / GAME 447 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 26930 / GAME 448 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 27005 / GAME 449 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [2830125568.0]\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "TIME 27080 / GAME 450 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 27111 / GAME 451 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 27186 / GAME 452 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "Episode Finish  [  0.  45. 100.]\n",
      "TIME 27261 / GAME 453 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [4118838272.0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "    with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/rocket_basic.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO5)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.set_window_visible(True)\n",
    "        game.set_mode(vzd.Mode.PLAYER)\n",
    "        game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "        game.get_available_buttons_size()\n",
    "        game.init()\n",
    "\n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "\n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 45\n",
    "    # Convert image into Black and white\n",
    "        img_channels = 4 # We stack 4 frames\n",
    "\n",
    "    # C51\n",
    "        num_atoms = 51\n",
    "\n",
    "        state_size = (img_rows, img_cols, img_channels)\n",
    "        agent = C51Agent(state_size, action_size, num_atoms)\n",
    "\n",
    "        agent.model = value_distribution_network(state_size, num_atoms, action_size, agent.learning_rate)\n",
    "        agent.target_model = value_distribution_network(state_size, num_atoms, action_size, agent.learning_rate)\n",
    "\n",
    "        x_t = game_state.screen_buffer # 480 x 640\n",
    "        x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "        s_t = np.stack(([x_t]*4), axis=2)    # It becomes 64x64x4\n",
    "        s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "    # Start training\n",
    "        epsilon = agent.initial_epsilon\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "        life = 0\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "\n",
    "            loss = 0\n",
    "            r_t = 0\n",
    "            a_t = np.zeros([action_size])\n",
    "\n",
    "        # Epsilon Greedy\n",
    "            action_idx  = agent.get_action(s_t)\n",
    "            a_t[action_idx] = 1\n",
    "\n",
    "            a_t = a_t.astype(int)\n",
    "            game.set_action(a_t.tolist())\n",
    "            skiprate = agent.frame_per_action\n",
    "            game.advance_action(skiprate)\n",
    "\n",
    "            game_state = game.get_state()  # Observe again after we take the action\n",
    "            is_terminated = game.is_episode_finished()\n",
    "\n",
    "            r_t = game.get_last_reward()  #each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "\n",
    "            if (is_terminated):\n",
    "                if (life > max_life):\n",
    "                    max_life = life\n",
    "                GAME += 1\n",
    "                life_buffer.append(life)\n",
    "                ammo_buffer.append(misc[1])\n",
    "                kills_buffer.append(misc[0])\n",
    "                print (\"Episode Finish \", misc)\n",
    "                game.new_episode()\n",
    "                game_state = game.get_state()\n",
    "                misc = game_state.game_variables\n",
    "                x_t1 = game_state.screen_buffer\n",
    "\n",
    "            x_t1 = game_state.screen_buffer\n",
    "            misc = game_state.game_variables\n",
    "\n",
    "            x_t1 = preprocessImg(x_t1, size=(img_rows, img_cols))\n",
    "            x_t1 = np.reshape(x_t1, (1, img_rows, img_cols, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "            r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            if (is_terminated):\n",
    "                life = 0\n",
    "            else:\n",
    "                life += 1\n",
    "\n",
    "        #update the cache\n",
    "            prev_misc = misc\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory and decrease epsilon\n",
    "            agent.replay_memory(s_t, action_idx, r_t, s_t1, is_terminated, t)\n",
    "\n",
    "        # Do the training\n",
    "            if t > agent.observe and t % agent.timestep_per_train == 0:\n",
    "                loss = agent.train_replay()\n",
    "\n",
    "            s_t = s_t1\n",
    "            t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                print(\"Now we save model\")\n",
    "                agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/c51_ddqn.h5\")\n",
    "                agent.model.save_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/c51_ddqn.h5\",\n",
    "                                         overwrite=True)\n",
    "\n",
    "        # print info\n",
    "            state = \"\"\n",
    "            if t <= agent.observe:\n",
    "                state = \"observe\"\n",
    "            elif t > agent.observe and t <= agent.observe + agent.explore:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "\n",
    "            if (is_terminated):\n",
    "                print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \\\n",
    "                      \"/ EPSILON\", agent.epsilon, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \\\n",
    "                      \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "            # Save Agent's Performance Statistics\n",
    "                if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                    print(\"Update Rolling Statistics\")\n",
    "                    agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                    agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                    agent.mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                    agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "\n",
    "                # Reset rolling stats buffer\n",
    "                    life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "                # Write Rolling Statistics to file\n",
    "                    with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/c51_ddqn_stats.txt\", \"a+\") as stats_file:\n",
    "                        stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                        stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                        stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                        stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                        stats_file.write('mavg_ammo_left: ' + str(agent.mavg_ammo_left) + '\\n')\n",
    "                        stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete\")\n",
    "endtime = time()\n",
    "print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
