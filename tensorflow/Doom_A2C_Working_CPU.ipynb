{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import backend as K\n",
    "import vizdoom as vzd\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 2) # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img, size)\n",
    "    img = skimage.color.rgb2gray(img) \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_network(input_shape, action_size, learning_rate):\n",
    "        \"\"\"Actor Network for A2C\n",
    "        \"\"\"\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(32, 8, strides=(4,4), input_shape=(input_shape)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Conv2D(64, 4, strides=(2,2)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Conv2D(64, 3, 3, padding=\"same\"))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(64))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(32))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Dense(action_size, activation='softmax'))\n",
    "\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=adam)\n",
    "        print(\"Actor Model\")\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_network(input_shape, value_size, learning_rate):\n",
    "        \"\"\"Critic Network for A2C\n",
    "        \"\"\"\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(32, 8, strides=(4,4), input_shape=(input_shape)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Conv2D(64, 4, strides=(2,2)))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Conv2D(64, 3, 3, padding=\"same\"))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(64))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Dense(32))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "        model.add(tf.keras.layers.Dense(value_size, activation='linear'))\n",
    "\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print(\"Critic Model\")\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class A2CAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "        self.observe = 0\n",
    "        self.frame_per_action = 4\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.gamma = 0.99\n",
    "        self.actor_lr = 0.0001\n",
    "        self.critic_lr = 0.0001\n",
    "\n",
    "        # Model for policy and critic network\n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "\n",
    "        # lists for the states, actions and rewards\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_ammo_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "\n",
    "    # using the output of policy network, pick action stochastically (Stochastic Policy)\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0], policy\n",
    "\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    # Use TD(1) i.e. Monte Carlo updates \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            if rewards[t] != 0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self):\n",
    "        episode_length = len(self.states)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(self.rewards)\n",
    "        # Standardized discounted rewards\n",
    "        discounted_rewards -= np.mean(discounted_rewards) \n",
    "        if np.std(discounted_rewards):\n",
    "            discounted_rewards /= np.std(discounted_rewards)\n",
    "        else:\n",
    "            self.states, self.actions, self.rewards = [], [], []\n",
    "            print ('std = 0!')\n",
    "            return 0\n",
    "\n",
    "        update_inputs = np.zeros(((episode_length,) + self.state_size)) # Episode_lengthx64x64x4\n",
    "\n",
    "        # Episode length is like the minibatch size in DQN\n",
    "        for i in range(episode_length):\n",
    "            update_inputs[i,:,:,:] = self.states[i]\n",
    "\n",
    "        # Prediction of state values for each state appears in the episode\n",
    "        values = self.critic.predict(update_inputs)\n",
    "\n",
    "        # Similar to one-hot target but the \"1\" is replaced by Advantage Function i.e. discounted_rewards R_t - Value\n",
    "        advantages = np.zeros((episode_length, self.action_size))\n",
    "\n",
    "        for i in range(episode_length):\n",
    "            advantages[i][self.actions[i]] = discounted_rewards[i] - values[i]\n",
    "        \n",
    "        actor_loss = self.actor.fit(update_inputs, advantages, epochs=1, verbose=0)\n",
    "        critic_loss = self.critic.fit(update_inputs, discounted_rewards, epochs=1, verbose=0)\n",
    "\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        return actor_loss.history['loss'], critic_loss.history['loss']\n",
    "\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 300\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use ammo\n",
    "            r_t = r_t - 10\n",
    "\n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        return r_t\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.actor.save_weights(name + \"_actor.h5\", overwrite=True)\n",
    "        self.critic.save_weights(name + \"_critic.h5\", overwrite=True)\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.actor.load_weights(name + \"_actor.h5\", overwrite=True)\n",
    "        self.critic.load_weights(name + \"_critic.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "\n",
    "    with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/rocket_basic.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO5)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.set_window_visible(True)\n",
    "        game.set_mode(vzd.Mode.PLAYER)\n",
    "        game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "        game.get_available_buttons_size()\n",
    "        game.init()\n",
    "\n",
    "    # Maximum number of episodes\n",
    "        max_episodes = 128 #1000000\n",
    "\n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "\n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 45\n",
    "    # Convert image into Black and white\n",
    "        img_channels = 4 # We stack 4 frames\n",
    "\n",
    "        state_size = (img_rows, img_cols, img_channels)\n",
    "        agent = A2CAgent(state_size, action_size)\n",
    "        agent.actor = actor_network(state_size, action_size, agent.actor_lr)\n",
    "        agent.critic = critic_network(state_size, agent.value_size, agent.critic_lr)\n",
    "\n",
    "    # Start training\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "        for i in range(max_episodes):\n",
    "\n",
    "            game.new_episode()\n",
    "            game_state = game.get_state()\n",
    "            misc = game_state.game_variables \n",
    "            prev_misc = misc\n",
    "\n",
    "            x_t = game_state.screen_buffer # 480 x 640\n",
    "            x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "            s_t = np.stack(([x_t]*4), axis=2) # It becomes 64x64x4\n",
    "            s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "            life = 0 # Episode life\n",
    "\n",
    "            while not game.is_episode_finished():\n",
    "\n",
    "                loss = 0 # Training Loss at each update\n",
    "                r_t = 0 # Initialize reward at time t\n",
    "                a_t = np.zeros([action_size]) # Initialize action at time t\n",
    "\n",
    "                x_t = game_state.screen_buffer\n",
    "                x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "                x_t = np.reshape(x_t, (1, img_rows, img_cols, 1))\n",
    "                s_t = np.append(x_t, s_t[:, :, :, :3], axis=3)\n",
    "                \n",
    "            # Sample action from stochastic softmax policy\n",
    "                action_idx, policy  = agent.get_action(s_t)\n",
    "                a_t[action_idx] = 1 \n",
    "\n",
    "                a_t = a_t.astype(int)\n",
    "                game.set_action(a_t.tolist())\n",
    "                skiprate = agent.frame_per_action # Frame Skipping = 4\n",
    "                game.advance_action(skiprate)\n",
    "\n",
    "                r_t = game.get_last_reward()  # Each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "            # Check if episode is terminated\n",
    "                is_terminated = game.is_episode_finished()\n",
    "\n",
    "                if (is_terminated):\n",
    "                # Save max_life\n",
    "                    if (life > max_life):\n",
    "                        max_life = life \n",
    "                    life_buffer.append(life)\n",
    "                    ammo_buffer.append(misc[1])\n",
    "                    kills_buffer.append(misc[0])\n",
    "                    print (\"Episode Finish \", prev_misc, policy)\n",
    "                else:\n",
    "                    life += 1\n",
    "                    game_state = game.get_state()  # Observe again after we take the action\n",
    "                    misc = game_state.game_variables\n",
    "\n",
    "            # Reward Shaping\n",
    "                r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            # Save trajactory sample <s, a, r> to the memory\n",
    "                agent.append_sample(s_t, action_idx, r_t)\n",
    "\n",
    "            # Update the cache\n",
    "                t += 1\n",
    "                prev_misc = misc\n",
    "\n",
    "                if (is_terminated and t > agent.observe):\n",
    "                # Every episode, agent learns from sample returns\n",
    "                    loss = agent.train_model()\n",
    "\n",
    "            # Save model every 10000 iterations\n",
    "                if t % 32 == 0: #10000 == 0:\n",
    "                    print(\"Save model\")\n",
    "                    agent.save_model(\"/home/spillingvoid/Downloads/programs/Doom/model/a2c\")\n",
    "\n",
    "                state = \"\"\n",
    "                if t <= agent.observe:\n",
    "                    state = \"Observe mode\"\n",
    "                else:\n",
    "                    state = \"Train mode\"\n",
    "\n",
    "                if (is_terminated):\n",
    "\n",
    "                # Print performance statistics at every episode end\n",
    "                    print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "                # Save Agent's Performance Statistics\n",
    "                    if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                        print(\"Update Rolling Statistics\")\n",
    "                        agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                        agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                        agent.mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                        agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "\n",
    "                    # Reset rolling stats buffer\n",
    "                        life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "                    # Write Rolling Statistics to file\n",
    "                        with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/a2c_stats.txt\", \"a+\") as stats_file:\n",
    "                            stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                            stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                            stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                            stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                            stats_file.write('mavg_ammo_left: ' + str(agent.mavg_ammo_left) + '\\n')\n",
    "                            stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')\n",
    "        \n",
    "        # Episode Finish. Increment game count\n",
    "            GAME += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete\")\n",
    "endtime = time()\n",
    "print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
