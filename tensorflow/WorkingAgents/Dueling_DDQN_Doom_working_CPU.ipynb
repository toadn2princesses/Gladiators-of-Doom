{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/skimage/viewer/utils/__init__.py:1: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  from .core import *\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "#from skimage.viewer.core import *\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import backend as K\n",
    "import vizdoom as vzd\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dueling_dqn(input_shape, action_size, learning_rate):\n",
    "\n",
    "        state_input = tf.keras.Input(shape=(input_shape))\n",
    "        x = tf.keras.layers.Conv2D(32, 8, 4, activation='relu')(state_input)\n",
    "        x = tf.keras.layers.Conv2D(64, 4, 2, activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(64, 3, 3, activation='relu', padding=\"same\")(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "        # state value tower - V\n",
    "        state_value = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        state_value = tf.keras.layers.Dense(1)(state_value)\n",
    "        state_value = tf.keras.layers.Lambda(lambda s: K.expand_dims(s[:, 0], -1),\n",
    "                                             output_shape=(action_size,))(state_value)\n",
    "\n",
    "        # action advantage tower - A\n",
    "        action_advantage = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        action_advantage = tf.keras.layers.Dense(action_size)(action_advantage)\n",
    "        action_advantage = tf.keras.layers.Lambda(lambda a: a[:, :] - K.mean(a[:, :], \n",
    "                                    keepdims=True), output_shape=(action_size,))(action_advantage)\n",
    "\n",
    "        # merge to state-action value function Q\n",
    "        #state_action_value = ([state_value + action_advantage])\n",
    "        state_action_value = (state_value + action_advantage)\n",
    "\n",
    "        model = Model(state_input, state_action_value)\n",
    "        #model.compile(rmsprop(lr=learning_rate), \"mse\")\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 2)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img,size)\n",
    "    img = skimage.color.rgb2gray(img)\n",
    "\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        #self.epochs = 2\n",
    "        self.batch_size = 4 #32\n",
    "        self.observe = 16 #5000\n",
    "        self.explore = 12 #50000 \n",
    "        self.frame_per_action = 4\n",
    "        self.update_target_freq =8 #3000 \n",
    "        self.timestep_per_train = 8 #100 # Number of timesteps between training interval\n",
    "            #times\n",
    "            #batch 4, observe 8 explore 8 frames 4 update freq 8 timestep 8 maxgame  4 time[]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000) #2000\n",
    "        self.max_memory = 50000 # number of previous transitions to remember\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 10 # window size for computing rolling statistics #50\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_ammo_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        After some time interval update the target model to be same with model\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action from model using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_idx = random.randrange(self.action_size)\n",
    "        else:\n",
    "            q = self.model.predict(state)\n",
    "            action_idx = np.argmax(q)\n",
    "            #print(\"action IDX\", action_idx)\n",
    "        return action_idx\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 1\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use ammo\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        return r_t\n",
    "\n",
    "    # Save trajectory sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, s_t, action_idx, r_t, s_t1, is_terminated, t):\n",
    "        self.memory.append((s_t, action_idx, r_t, s_t1, is_terminated))\n",
    "        if self.epsilon > self.final_epsilon and t > self.observe:\n",
    "            self.epsilon -= (self.initial_epsilon - self.final_epsilon) / self.explore\n",
    "\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.popleft()\n",
    "\n",
    "        # Update the target model to be same with model\n",
    "        if t % self.update_target_freq == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_minibatch_replay(self):\n",
    "        \"\"\"\n",
    "        Train on a single minibatch\n",
    "        \"\"\"\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros(((batch_size,) + self.state_size)) # Shape 64, img_rows, img_cols, 4\n",
    "        update_target = np.zeros(((batch_size,) + self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            update_input[i,:,:,:] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i,:,:,:] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) # Shape 64, Num_Actions\n",
    "\n",
    "        target_val = self.model.predict(update_target)\n",
    "        target_val_ = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_val[i])\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (target_val_[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        loss = self.model.train_on_batch(update_input, target)\n",
    "\n",
    "        return np.max(target[-1]), loss\n",
    "\n",
    "    # Pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "\n",
    "        num_samples = min(self.batch_size * self.timestep_per_train, len(self.memory))\n",
    "        replay_samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "        update_input = np.zeros(((num_samples,) + self.state_size)) \n",
    "        update_target = np.zeros(((num_samples,) + self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            update_input[i,:,:,:] = replay_samples[i][0]\n",
    "            action.append(replay_samples[i][1])\n",
    "            reward.append(replay_samples[i][2])\n",
    "            update_target[i,:,:,:] = replay_samples[i][3]\n",
    "            done.append(replay_samples[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input) \n",
    "        target_val = self.model.predict(update_target)\n",
    "        target_val_ = self.target_model.predict(update_target)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                a = np.argmax(target_val[i])\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (target_val_[i][a])\n",
    "        callbacks = [tf.keras.callbacks.TensorBoard(\n",
    "                            log_dir=\"/home/spillingvoid/Downloads/programs/Doom/statistics/\", \n",
    "                            histogram_freq=1, write_graph=True, write_images=True, embeddings_freq=1),\n",
    "                tf.keras.callbacks.ModelCheckpoint(filepath=\"/home/spillingvoid/Downloads/programs/Doom/models/Dueling_ddqn_weights\",\n",
    "                save_weights_only=True),\n",
    "                \n",
    "                ]\n",
    "        loss = self.model.fit(update_input, target, batch_size=self.batch_size, callbacks=callbacks, \n",
    "                              epochs=1, verbose=2)\n",
    "        Q_max = np.max(target[-1]) \n",
    "        print(\"Q_MAX\", Q_max)\n",
    "        return Q_max, loss.history['loss']#np.max(target[-1])\n",
    "\n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\")\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\")\n",
    "        self.model.save_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishing():\n",
    "    print(\"saving model\")\n",
    "    agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)\n",
    "    print(\"Save Complete\")\n",
    "    game.close()\n",
    "    print(\"======================================\")\n",
    "    print(\"Training is finished.\")\n",
    "    print(\"Training complete\")\n",
    "    endtime = time()\n",
    "    print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))\n",
    "    quit\n",
    "    sys.exit()\n",
    "    %reset -f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 45, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 6, 10, 32)    8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 4, 64)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 2, 64)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          33024       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          33024       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            771         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 3)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 3)            0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 145,060\n",
      "Trainable params: 145,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30, 45, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 10, 32)    8224        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 4, 64)     32832       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 1, 2, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          33024       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          33024       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3)            771         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 3)            0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 3)            0           lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 145,060\n",
      "Trainable params: 145,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-44a8d3d7f5a7>:5: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  img = skimage.color.rgb2gray(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 2s - loss: 8.7335\n",
      "Q_MAX 0.03090651\n",
      "8/8 - 2s - loss: 8.5942\n",
      "Q_MAX -0.050845064\n",
      "8/8 - 0s - loss: 7.8059\n",
      "Q_MAX -0.18831763\n",
      "8/8 - 0s - loss: 7.5382\n",
      "Q_MAX -0.405976\n",
      "8/8 - 0s - loss: 5.0687\n",
      "Q_MAX -0.72257495\n",
      "8/8 - 0s - loss: 6.3341\n",
      "Q_MAX -1.5948902\n",
      "8/8 - 0s - loss: 5.6891\n",
      "Q_MAX -3.5841951\n",
      "Episode Finish  [  0.  43. 100.]\n",
      "TIME 75 / GAME 1 / STATE train / EPSILON -0.08322499999999981 / ACTION 0 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "8/8 - 0s - loss: 5.1837\n",
      "Q_MAX -5.078415\n",
      "8/8 - 0s - loss: 3.4315\n",
      "Q_MAX -6.115849\n",
      "8/8 - 0s - loss: 3.4187\n",
      "Q_MAX -7.7979655\n",
      "Now we save model\n",
      "8/8 - 0s - loss: 5.0237\n",
      "Q_MAX -12.258146\n",
      "8/8 - 0s - loss: 4.7442\n",
      "Q_MAX -14.030409\n",
      "8/8 - 0s - loss: 4.0089\n",
      "Q_MAX -16.485273\n",
      "8/8 - 0s - loss: 3.8104\n",
      "Q_MAX -15.27371\n",
      "8/8 - 0s - loss: 2.9870\n",
      "Q_MAX -17.47933\n",
      "8/8 - 0s - loss: 4.6967\n",
      "Q_MAX -18.007727\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "TIME 150 / GAME 2 / STATE train / EPSILON -0.08322499999999981 / ACTION 1 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "8/8 - 0s - loss: 4.8174\n",
      "Q_MAX -20.176725\n",
      "8/8 - 0s - loss: 13.4273\n",
      "Q_MAX -22.403864\n",
      "8/8 - 0s - loss: 5.0318\n",
      "Q_MAX -28.17861\n",
      "8/8 - 0s - loss: 11.9575\n",
      "Q_MAX -23.547777\n",
      "8/8 - 0s - loss: 5.4603\n",
      "Q_MAX -33.7134\n",
      "8/8 - 0s - loss: 4.0093\n",
      "Q_MAX -26.780922\n",
      "Now we save model\n",
      "8/8 - 0s - loss: 5.9875\n",
      "Q_MAX -30.667103\n",
      "8/8 - 0s - loss: 4.7172\n",
      "Q_MAX -30.37109\n",
      "8/8 - 0s - loss: 4.2399\n",
      "Q_MAX -31.654055\n",
      "Episode Finish  [  0.  46. 100.]\n",
      "8/8 - 0s - loss: 3.5907\n",
      "Q_MAX -36.312447\n",
      "TIME 225 / GAME 3 / STATE train / EPSILON -0.08322499999999981 / ACTION 1 / REWARD -4.0 / Q_MAX -3.631245e+01 / LIFE 74 / LOSS [3.590698003768921]\n",
      "8/8 - 0s - loss: 4.8775\n",
      "Q_MAX -45.253716\n",
      "8/8 - 0s - loss: 5.2405\n",
      "Q_MAX -36.21687\n",
      "8/8 - 0s - loss: 4.3870\n",
      "Q_MAX -40.529587\n",
      "8/8 - 0s - loss: 5.7399\n",
      "Q_MAX -39.475574\n",
      "8/8 - 0s - loss: 22.4946\n",
      "Q_MAX -52.71689\n",
      "8/8 - 0s - loss: 27.8238\n",
      "Q_MAX -51.56883\n",
      "8/8 - 0s - loss: 3.0582\n",
      "Q_MAX -48.043724\n",
      "8/8 - 0s - loss: 41.8174\n",
      "Q_MAX -52.29335\n",
      "8/8 - 0s - loss: 3.5325\n",
      "Q_MAX -53.197304\n",
      "Episode Finish  [  0.  47. 100.]\n",
      "Now we save model\n",
      "TIME 300 / GAME 4 / STATE train / EPSILON -0.08322499999999981 / ACTION 1 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "8/8 - 0s - loss: 3.5559\n",
      "Q_MAX -56.414845\n",
      "8/8 - 0s - loss: 3.1585\n",
      "Q_MAX -45.767323\n",
      "8/8 - 0s - loss: 2.6052\n",
      "Q_MAX -49.03491\n",
      "8/8 - 0s - loss: 25.5578\n",
      "Q_MAX -57.51688\n",
      "8/8 - 0s - loss: 3.5754\n",
      "Q_MAX -47.568718\n",
      "8/8 - 0s - loss: 25.9953\n",
      "Q_MAX -61.781662\n",
      "8/8 - 0s - loss: 2.5581\n",
      "Q_MAX -53.77484\n",
      "8/8 - 0s - loss: 2.1901\n",
      "Q_MAX -55.026012\n",
      "8/8 - 0s - loss: 4.2508\n",
      "Q_MAX -57.750046\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 375 / GAME 5 / STATE train / EPSILON -0.08322499999999981 / ACTION 0 / REWARD -4.0 / Q_MAX 0.000000e+00 / LIFE 74 / LOSS 0\n",
      "saving model\n",
      "Save Complete\n",
      "======================================\n",
      "Training is finished.\n",
      "Training complete\n",
      " Test Time elapsed: 2.75 minutes\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "    \n",
    "     with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/rocket_basic.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO5)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.set_window_visible(True)\n",
    "        game.set_mode(vzd.Mode.PLAYER)\n",
    "        game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "        game.get_available_buttons_size()\n",
    "        game.init()\n",
    "\n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "    \n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 45\n",
    "    # Convert image into Black and white\n",
    "        img_channels = 4 # We stack 4 frames\n",
    "\n",
    "        state_size = (img_rows, img_cols, img_channels)\n",
    "        agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "        agent.model = dueling_dqn(state_size, action_size, agent.learning_rate)\n",
    "        agent.target_model = dueling_dqn(state_size, action_size, agent.learning_rate)\n",
    "\n",
    "        x_t = game_state.screen_buffer # 480 x 640\n",
    "        x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "        s_t = np.stack(([x_t]*4), axis=2) # It becomes 64x64x4\n",
    "        s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "    # Start training\n",
    "        maxGAME = 4\n",
    "        epsilon = agent.initial_epsilon\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "        life = 0\n",
    "        #maxGAME = 0\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, ammo_buffer, kills_buffer = [], [], []\n",
    "        \n",
    "        \n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables \n",
    "        prev_misc = misc\n",
    "            \n",
    "                \n",
    "        while not game.is_episode_finished():\n",
    "            if GAME > maxGAME:\n",
    "                    finishing()\n",
    "                    \n",
    "            loss = 0\n",
    "            Q_max = 0\n",
    "            r_t = 0\n",
    "            a_t = np.zeros([action_size])\n",
    "\n",
    "        # Epsilon Greedy\n",
    "            action_idx  = agent.get_action(s_t)\n",
    "            a_t[action_idx] = 1\n",
    "\n",
    "            a_t = a_t.astype(int)\n",
    "            game.set_action(a_t.tolist())\n",
    "            skiprate = agent.frame_per_action\n",
    "            game.advance_action(skiprate)\n",
    "\n",
    "            game_state = game.get_state()  # Observe again after we take the action\n",
    "            is_terminated = game.is_episode_finished()\n",
    "\n",
    "            r_t = game.get_last_reward()  #each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "\n",
    "            if (is_terminated):\n",
    "                if (life > max_life):\n",
    "                    max_life = life\n",
    "                GAME += 1\n",
    "                life_buffer.append(life)\n",
    "                ammo_buffer.append(misc[1])\n",
    "                kills_buffer.append(misc[0])\n",
    "                print (\"Episode Finish \", misc)\n",
    "                game.new_episode()\n",
    "                game_state = game.get_state()\n",
    "                misc = game_state.game_variables\n",
    "                x_t1 = game_state.screen_buffer\n",
    "\n",
    "            x_t1 = game_state.screen_buffer\n",
    "            misc = game_state.game_variables\n",
    "\n",
    "            x_t1 = preprocessImg(x_t1, size=(img_rows, img_cols))\n",
    "            x_t1 = np.reshape(x_t1, (1, img_rows, img_cols, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "            r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            if (is_terminated):\n",
    "                life = 0\n",
    "            else:\n",
    "                life += 1\n",
    "\n",
    "        # Update the cache\n",
    "            prev_misc = misc\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory and decrease epsilon\n",
    "            agent.replay_memory(s_t, action_idx, r_t, s_t1, is_terminated, t)\n",
    "\n",
    "        # Do the training\n",
    "            if t > agent.observe and t % agent.timestep_per_train == 0:\n",
    "                Q_max, loss = agent.train_replay()\n",
    "            \n",
    "            s_t = s_t1\n",
    "            t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "            if t % 100 == 0:\n",
    "                print(\"Now we save model\")\n",
    "                agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)\n",
    "                agent.model.save_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)\n",
    "\n",
    "        # print info\n",
    "            #state = \"\"\n",
    "            if t <= agent.observe:\n",
    "                state = \"observe\"\n",
    "            elif t > agent.observe and t <= agent.observe + agent.explore:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "\n",
    "            if (is_terminated):\n",
    "                print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \\\n",
    "                        \"/ EPSILON\", agent.epsilon, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \\\n",
    "                        \"/ Q_MAX %e\" % Q_max, \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "                    \n",
    "            # Save Agent's Performance Statistics\n",
    "                if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                    print(\"Update Rolling Statistics\")\n",
    "                    agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                    agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                    agent.mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                    agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "\n",
    "                # Reset rolling stats buffer\n",
    "                    life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "                # Write Rolling Statistics to file\n",
    "                    with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/dueling_ddqn_stats.txt\", \"a+\") as stats_file:\n",
    "                        stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                        stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                        stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                        stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                        stats_file.write('mavg_ammo_left: ' + str(agent.mavg_ammo_left) + '\\n')\n",
    "                        stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')\n",
    "                    \n",
    "            \n",
    "                \n",
    "                \n",
    "                    #agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
