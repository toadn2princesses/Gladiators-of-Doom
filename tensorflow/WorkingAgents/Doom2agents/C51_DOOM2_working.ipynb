{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/skimage/viewer/utils/__init__.py:1: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  from .core import *\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import math\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import Model\n",
    "from keras import backend as K\n",
    "import vizdoom as vzd\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 2)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img,size)\n",
    "    img = skimage.color.rgb2gray(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C51Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, num_atoms):\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the DQN\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.epsilon = 1.0\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.batch_size = 4 #32\n",
    "        self.observe = 32 #2000\n",
    "        self.explore = 64 # 50000\n",
    "        self.frame_per_action = 4\n",
    "        self.update_target_freq = 40 #3000 \n",
    "        self.timestep_per_train = 4 #100 # Number of timesteps between training interval\n",
    "        self.maxGAME = 64\n",
    "        # Initialize Atoms\n",
    "        self.num_atoms = num_atoms # 51 for C51\n",
    "        self.v_max = 300 # Max possible score for Defend the center is 26 - 0.1*26 = 23.4\n",
    "        self.v_min = -600 # -0.1*26 - 1 = -3.6\n",
    "        self.delta_z = (self.v_max - self.v_min) / float(self.num_atoms - 1)\n",
    "        self.z = [self.v_min + i * self.delta_z for i in range(self.num_atoms)]\n",
    "\n",
    "        # Create replay memory using deque\n",
    "        self.memory = deque()\n",
    "        self.max_memory = 50000 # number of previous transitions to remember\n",
    "\n",
    "        # Models for value distribution\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_pistol_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "        self.mavg_shotgun_left = []\n",
    "        self.mavg_minigun_left = []\n",
    "        self.mavg_plasma_left = []\n",
    "        self.mavg_rocket_left = []\n",
    "        self.mavg_secret_left = []\n",
    "        self.mavg_hit_count = []\n",
    "        self.mavg_damage_given = []\n",
    "        self.mavg_item_collected = []\n",
    "        self.mavg_armor = []\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        After some time interval update the target model to be same with model\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Get action from model using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action_idx = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action_idx = self.get_optimal_action(state)\n",
    "\n",
    "        return action_idx\n",
    "\n",
    "    def get_optimal_action(self, state):\n",
    "        \"\"\"Get optimal action for a state\n",
    "        \"\"\"\n",
    "        z = self.model.predict(state) # Return a list [1x51, 1x51, 1x51]\n",
    "\n",
    "        z_concat = np.vstack(z)\n",
    "        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1) \n",
    "\n",
    "        # Pick action with the biggest Q value\n",
    "        action_idx = np.argmax(q)\n",
    "        \n",
    "        return action_idx\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 310\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use pistol\n",
    "            r_t = r_t - 1\n",
    "        if (misc[1] > prev_misc[1]):\n",
    "            r_t = r_t + 1\n",
    "        \n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 2\n",
    "        if (misc[2] > prev_misc[2]):\n",
    "            r_t = r_t + 2.25\n",
    "        \n",
    "        if (misc[3] < prev_misc[3]): # Loss shotgun\n",
    "            r_t = r_t - 3\n",
    "        if (misc[3] < prev_misc[3]):\n",
    "            r_t = r_t +3\n",
    "        \n",
    "        if (misc[4] < prev_misc[4]): # Loss minigun\n",
    "            r_t = r_t - 1\n",
    "        if (misc[4] < prev_misc[4]):\n",
    "            r_t = r_t + 1\n",
    "        \n",
    "        if (misc[5] < prev_misc[5]): # plasma\n",
    "            r_t = r_t - 2\n",
    "        if (misc[5] < prev_misc[5]):\n",
    "            r_t = r_t + 2\n",
    "            \n",
    "        if (misc[6] < prev_misc[6]): # rocket\n",
    "            r_t = r_t - 5\n",
    "        if (misc[6] > prev_misc[6]):\n",
    "            r_t = r_t + 5\n",
    "            \n",
    "        if (misc[7] > prev_misc[7]): # secrets\n",
    "            r_t = r_t + 15 \n",
    "                    \n",
    "        if (misc[8] > prev_misc[8]): # hitcount\n",
    "            r_t = r_t + 5\n",
    "            \n",
    "        if (misc[9] > prev_misc[9]): # hits taken\n",
    "            r_t = r_t - 5\n",
    "        \n",
    "        if (misc[10] > prev_misc[10]): # items picked up\n",
    "            r_t = r_t + 2\n",
    "        \n",
    "        if (misc[11] < prev_misc[11]): # armor\n",
    "            r_t = r_t - 1\n",
    "        if (misc[11] > prev_misc[11]):\n",
    "            r_t = r_t + 1.5\n",
    "        \n",
    "\n",
    "        return r_t\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, s_t, action_idx, r_t, s_t1, is_terminated, t):\n",
    "        self.memory.append((s_t, action_idx, r_t, s_t1, is_terminated))\n",
    "        if self.epsilon > self.final_epsilon and t > self.observe:\n",
    "            self.epsilon -= (self.initial_epsilon - self.final_epsilon) / self.explore\n",
    "\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.popleft()\n",
    "\n",
    "        # Update the target model to be same with model\n",
    "        if t % self.update_target_freq == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "\n",
    "        num_samples = min(self.batch_size * self.timestep_per_train, len(self.memory))\n",
    "        replay_samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "        state_inputs = np.zeros(((num_samples,) + self.state_size)) \n",
    "        next_states = np.zeros(((num_samples,) + self.state_size)) \n",
    "        m_prob = [np.zeros((num_samples, self.num_atoms)) for i in range(action_size)]\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            state_inputs[i,:,:,:] = replay_samples[i][0]\n",
    "            action.append(replay_samples[i][1])\n",
    "            reward.append(replay_samples[i][2])\n",
    "            next_states[i,:,:,:] = replay_samples[i][3]\n",
    "            done.append(replay_samples[i][4])\n",
    "\n",
    "        z = self.model.predict(next_states) # Return a list [32x51, 32x51, 32x51]\n",
    "        z_ = self.model.predict(next_states) # Return a list [32x51, 32x51, 32x51]\n",
    "\n",
    "        # Get Optimal Actions for the next states (from distribution z)\n",
    "        optimal_action_idxs = []\n",
    "        z_concat = np.vstack(z)\n",
    "        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1) # length (num_atoms x num_actions)\n",
    "        q = q.reshape((num_samples, action_size), order='F')\n",
    "        optimal_action_idxs = np.argmax(q, axis=1)\n",
    "\n",
    "        # Project Next State Value Distribution (of optimal action) to Current State\n",
    "        for i in range(num_samples):\n",
    "            if done[i]: # Terminal State\n",
    "                # Distribution collapses to a single point\n",
    "                Tz = min(self.v_max, max(self.v_min, reward[i]))\n",
    "                bj = (Tz - self.v_min) / self.delta_z \n",
    "                m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                m_prob[action[i]][i][int(m_l)] += (m_u - bj)\n",
    "                m_prob[action[i]][i][int(m_u)] += (bj - m_l)\n",
    "            else:\n",
    "                for j in range(self.num_atoms):\n",
    "                    Tz = min(self.v_max, max(self.v_min, reward[i] + self.gamma * self.z[j]))\n",
    "                    bj = (Tz - self.v_min) / self.delta_z \n",
    "                    m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                    m_prob[action[i]][i][int(m_l)] += z_[optimal_action_idxs[i]][i][j] * (m_u - bj)\n",
    "                    m_prob[action[i]][i][int(m_u)] += z_[optimal_action_idxs[i]][i][j] * (bj - m_l)\n",
    "\n",
    "        callbacks = [tf.keras.callbacks.TensorBoard(log_dir=\"/home/spillingvoid/Downloads/programs/Doom/statistics/\", \n",
    "                                                    histogram_freq=1, write_graph=True),\n",
    "                tf.keras.callbacks.ModelCheckpoint(filepath=\"/home/spillingvoid/Downloads/programs/Doom/models/C51_weights\",\n",
    "                save_weights_only=True),\n",
    "                    ]\n",
    "        \n",
    "        loss = self.model.fit(state_inputs, m_prob, batch_size=self.batch_size, epochs=1, \n",
    "                              callbacks=callbacks, verbose=2)\n",
    "\n",
    "        return loss.history['loss']\n",
    "\n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/c51_ddqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_distribution_network(input_shape, num_atoms, action_size, learning_rate):\n",
    "        \"\"\"Model Value Distribution\n",
    "\n",
    "        With States as inputs and output Probability Distributions for all Actions\n",
    "        \"\"\"\n",
    "\n",
    "        state_input = tf.keras.Input(shape=(input_shape)) \n",
    "        cnn_feature = tf.keras.layers.Conv2D(32, 8, 4, activation='relu')(state_input)\n",
    "        cnn_feature = tf.keras.layers.Conv2D(64, 4, 2, activation='relu')(cnn_feature)\n",
    "        cnn_feature = tf.keras.layers.Conv2D(64, 3, 3, activation='relu', padding=\"same\")(cnn_feature)\n",
    "        cnn_feature = tf.keras.layers.Flatten()(cnn_feature)\n",
    "        cnn_feature = tf.keras.layers.Dense(512, activation='relu')(cnn_feature)\n",
    "\n",
    "        distribution_list = []\n",
    "        for i in range(action_size):\n",
    "            distribution_list.append(tf.keras.layers.Dense(num_atoms, activation='softmax')(cnn_feature))\n",
    "\n",
    "        model = Model(state_input, distribution_list)\n",
    "\n",
    "        adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=adam)\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishing():\n",
    "    print(\"saving model\")\n",
    "    agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)\n",
    "    print(\"Save Complete\")\n",
    "    game.close()\n",
    "    print(\"======================================\")\n",
    "    print(\"Training is finished.\")\n",
    "    print(\"Training complete\")\n",
    "    endtime = time()\n",
    "    print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))\n",
    "    quit\n",
    "    sys.exit()\n",
    "    %reset -f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 45, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 6, 10, 32)    8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 4, 64)     32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 2, 64)     36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          66048       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 51)           26163       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 51)           26163       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 51)           26163       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 222,521\n",
      "Trainable params: 222,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30, 45, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 10, 32)    8224        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 4, 64)     32832       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 1, 2, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          66048       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 51)           26163       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 51)           26163       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 51)           26163       dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 222,521\n",
      "Trainable params: 222,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-97d2ae62978c>:5: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  img = skimage.color.rgb2gray(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 2s - loss: 3.9223 - dense_1_loss: 0.9831 - dense_2_loss: 1.4744 - dense_3_loss: 1.4648\n",
      "4/4 - 0s - loss: 3.9128 - dense_1_loss: 1.4698 - dense_2_loss: 1.2287 - dense_3_loss: 1.2143\n",
      "4/4 - 0s - loss: 3.9225 - dense_1_loss: 1.4700 - dense_2_loss: 1.2287 - dense_3_loss: 1.2239\n",
      "4/4 - 0s - loss: 3.9228 - dense_1_loss: 1.7208 - dense_2_loss: 1.2239 - dense_3_loss: 0.9782\n",
      "4/4 - 0s - loss: 3.9282 - dense_1_loss: 1.7211 - dense_2_loss: 1.2239 - dense_3_loss: 0.9832\n",
      "4/4 - 0s - loss: 3.9137 - dense_1_loss: 0.9838 - dense_2_loss: 1.4692 - dense_3_loss: 1.4607\n",
      "4/4 - 0s - loss: 3.9338 - dense_1_loss: 1.4768 - dense_2_loss: 1.7191 - dense_3_loss: 0.7379\n",
      "4/4 - 0s - loss: 3.9348 - dense_1_loss: 0.7390 - dense_2_loss: 1.9647 - dense_3_loss: 1.2311\n",
      "4/4 - 0s - loss: 3.9361 - dense_1_loss: 0.7364 - dense_2_loss: 2.2131 - dense_3_loss: 0.9867\n",
      "4/4 - 0s - loss: 3.9330 - dense_1_loss: 1.9610 - dense_2_loss: 1.2298 - dense_3_loss: 0.7422\n",
      "Episode Finish  [  0.  42. 100.]\n",
      "TIME 75 / GAME 1 / STATE explore / EPSILON 0.3438156250000016 / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS 0\n",
      "4/4 - 0s - loss: 3.9255 - dense_1_loss: 1.4646 - dense_2_loss: 1.7150 - dense_3_loss: 0.7459\n",
      "4/4 - 0s - loss: 3.9102 - dense_1_loss: 1.7020 - dense_2_loss: 1.4677 - dense_3_loss: 0.7405\n",
      "4/4 - 0s - loss: 4.0779 - dense_1_loss: 1.8180 - dense_2_loss: 1.4919 - dense_3_loss: 0.7679\n",
      "4/4 - 0s - loss: 4.1985 - dense_1_loss: 2.4533 - dense_2_loss: 0.5232 - dense_3_loss: 1.2219\n",
      "4/4 - 0s - loss: 4.3636 - dense_1_loss: 2.8048 - dense_2_loss: 1.0829 - dense_3_loss: 0.4758\n",
      "4/4 - 0s - loss: 4.2600 - dense_1_loss: 0.5710 - dense_2_loss: 2.5192 - dense_3_loss: 1.1698\n",
      "4/4 - 0s - loss: 4.1688 - dense_1_loss: 1.7067 - dense_2_loss: 0.8377 - dense_3_loss: 1.6244\n",
      "4/4 - 0s - loss: 4.1166 - dense_1_loss: 1.1092 - dense_2_loss: 1.3867 - dense_3_loss: 1.6207\n",
      "4/4 - 0s - loss: 4.0082 - dense_1_loss: 1.3140 - dense_2_loss: 1.8425 - dense_3_loss: 0.8517\n",
      "4/4 - 0s - loss: 3.8597 - dense_1_loss: 1.7325 - dense_2_loss: 1.0007 - dense_3_loss: 1.1266\n",
      "4/4 - 0s - loss: 3.6780 - dense_1_loss: 1.3644 - dense_2_loss: 1.1937 - dense_3_loss: 1.1198\n",
      "4/4 - 0s - loss: 3.5008 - dense_1_loss: 0.8901 - dense_2_loss: 1.0664 - dense_3_loss: 1.5443\n",
      "4/4 - 0s - loss: 3.5988 - dense_1_loss: 0.9133 - dense_2_loss: 1.3725 - dense_3_loss: 1.3130\n",
      "4/4 - 0s - loss: 3.7632 - dense_1_loss: 1.8735 - dense_2_loss: 0.6898 - dense_3_loss: 1.1999\n",
      "4/4 - 0s - loss: 3.7408 - dense_1_loss: 1.3945 - dense_2_loss: 0.6805 - dense_3_loss: 1.6658\n",
      "4/4 - 0s - loss: 3.6935 - dense_1_loss: 0.4888 - dense_2_loss: 2.0246 - dense_3_loss: 1.1801\n",
      "4/4 - 0s - loss: 3.5054 - dense_1_loss: 0.8018 - dense_2_loss: 1.3323 - dense_3_loss: 1.3713\n",
      "4/4 - 0s - loss: 2.4339 - dense_1_loss: 0.8299 - dense_2_loss: 0.6723 - dense_3_loss: 0.9316\n",
      "Episode Finish  [  0.  40. 100.]\n",
      "TIME 147 / GAME 2 / STATE train / EPSILON -0.01552343749999845 / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "4/4 - 0s - loss: 3.3531 - dense_1_loss: 0.2613 - dense_2_loss: 1.1223 - dense_3_loss: 1.9694\n",
      "4/4 - 0s - loss: 3.4053 - dense_1_loss: 0.8737 - dense_2_loss: 1.2528 - dense_3_loss: 1.2789\n",
      "4/4 - 0s - loss: 3.4597 - dense_1_loss: 0.8881 - dense_2_loss: 1.4569 - dense_3_loss: 1.1148\n",
      "4/4 - 0s - loss: 3.4150 - dense_1_loss: 0.7066 - dense_2_loss: 1.4172 - dense_3_loss: 1.2911\n",
      "4/4 - 0s - loss: 3.3396 - dense_1_loss: 0.8447 - dense_2_loss: 1.6769 - dense_3_loss: 0.8180\n",
      "4/4 - 0s - loss: 3.5608 - dense_1_loss: 1.0714 - dense_2_loss: 0.8841 - dense_3_loss: 1.6053\n",
      "4/4 - 0s - loss: 3.9671 - dense_1_loss: 0.7516 - dense_2_loss: 0.7056 - dense_3_loss: 2.5098\n",
      "4/4 - 0s - loss: 3.7654 - dense_1_loss: 0.7103 - dense_2_loss: 0.8792 - dense_3_loss: 2.1759\n",
      "4/4 - 0s - loss: 3.6526 - dense_1_loss: 0.6468 - dense_2_loss: 1.5241 - dense_3_loss: 1.4816\n",
      "4/4 - 0s - loss: 3.9715 - dense_1_loss: 1.4583 - dense_2_loss: 1.1896 - dense_3_loss: 1.3237\n",
      "Episode Finish  [  0.  44. 100.]\n",
      "4/4 - 0s - loss: 4.2357 - dense_1_loss: 0.6277 - dense_2_loss: 0.8912 - dense_3_loss: 2.7167\n",
      "TIME 189 / GAME 3 / STATE train / EPSILON -0.01552343749999845 / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [4.235681056976318]\n",
      "4/4 - 0s - loss: 4.6420 - dense_1_loss: 0.3321 - dense_2_loss: 2.0731 - dense_3_loss: 2.2367\n",
      "4/4 - 0s - loss: 6.8146 - dense_1_loss: 0.6658 - dense_2_loss: 0.8290 - dense_3_loss: 5.3198\n",
      "4/4 - 0s - loss: 9.6524 - dense_1_loss: 1.7857 - dense_2_loss: 1.8830 - dense_3_loss: 5.9837\n",
      "4/4 - 0s - loss: 9.8500 - dense_1_loss: 3.2475 - dense_2_loss: 1.5216 - dense_3_loss: 5.0809\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "4/4 - 0s - loss: 9.5545 - dense_1_loss: 1.7881 - dense_2_loss: 3.0446 - dense_3_loss: 4.7217\n",
      "TIME 209 / GAME 4 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [9.554508209228516]\n",
      "4/4 - 0s - loss: 14.6392 - dense_1_loss: 4.6076 - dense_2_loss: 8.1910 - dense_3_loss: 1.8406\n",
      "4/4 - 0s - loss: 20.7724 - dense_1_loss: 1.1136 - dense_2_loss: 15.6466 - dense_3_loss: 4.0122\n",
      "4/4 - 0s - loss: 12.8676 - dense_1_loss: 4.6412 - dense_2_loss: 5.6212 - dense_3_loss: 2.6053\n",
      "4/4 - 0s - loss: 17.4150 - dense_1_loss: 6.0799 - dense_2_loss: 4.6442 - dense_3_loss: 6.6910\n",
      "4/4 - 0s - loss: 39.9863 - dense_1_loss: 4.7945 - dense_2_loss: 11.3063 - dense_3_loss: 23.8855\n",
      "4/4 - 0s - loss: 33.3556 - dense_1_loss: 13.7536 - dense_2_loss: 13.5166 - dense_3_loss: 6.0854\n",
      "4/4 - 0s - loss: 26.9982 - dense_1_loss: 11.9800 - dense_2_loss: 10.4396 - dense_3_loss: 4.5786\n",
      "Episode Finish  [  0.  48. 100.]\n",
      "TIME 239 / GAME 5 / STATE train / EPSILON -0.01552343749999845 / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS 0\n",
      "saving model\n",
      "Save Complete\n",
      "======================================\n",
      "Training is finished.\n",
      "Training complete\n",
      " Test Time elapsed: 1.78 minutes\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "    with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/Doom/scenarios/Doom32.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO3)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO4)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO5)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO6)\n",
    "        game.add_available_game_variable(vzd.GameVariable.SECRETCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HITCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HITS_TAKEN)\n",
    "        game.add_available_game_variable(vzd.GameVariable.ITEMCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.ARMOR)\n",
    "\n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "\n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 45\n",
    "    # Convert image into Black and white\n",
    "        img_channels = 4 # We stack 4 frames\n",
    "\n",
    "    # C51\n",
    "        num_atoms = 51\n",
    "\n",
    "        state_size = (img_rows, img_cols, img_channels)\n",
    "        agent = C51Agent(state_size, action_size, num_atoms)\n",
    "\n",
    "        agent.model = value_distribution_network(state_size, num_atoms, action_size, agent.learning_rate)\n",
    "        agent.target_model = value_distribution_network(state_size, num_atoms, action_size, agent.learning_rate)\n",
    "\n",
    "        x_t = game_state.screen_buffer # 480 x 640\n",
    "        x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "        s_t = np.stack(([x_t]*4), axis=2)    # It becomes 64x64x4\n",
    "        s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "        is_terminated = game.is_episode_finished()\n",
    "\n",
    "    # Start training\n",
    "        maxGAME = 4\n",
    "        epsilon = agent.initial_epsilon\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "        life = 0\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, pistol_buffer, kills_buffer, shotgun_buffer, minigun_buffer, plasma_buffer, rocket_buffer, secret_buffer, hit_buffer, damtaken_buffer, item_buffer, armor_buffer = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        while not game.is_episode_finished():\n",
    "            \n",
    "            if GAME > maxGAME:\n",
    "                finishing()\n",
    "                \n",
    "            loss = 0\n",
    "            r_t = 0\n",
    "            a_t = np.zeros([action_size])\n",
    "\n",
    "        # Epsilon Greedy\n",
    "            action_idx  = agent.get_action(s_t)\n",
    "            a_t[action_idx] = 1\n",
    "\n",
    "            a_t = a_t.astype(int)\n",
    "            game.set_action(a_t.tolist())\n",
    "            skiprate = agent.frame_per_action\n",
    "            game.advance_action(skiprate)\n",
    "\n",
    "            game_state = game.get_state()  # Observe again after we take the action\n",
    "            is_terminated = game.is_episode_finished()\n",
    "\n",
    "            r_t = game.get_last_reward()  #each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "\n",
    "            if (is_terminated):\n",
    "                if (life > max_life):\n",
    "                    max_life = life\n",
    "                GAME += 1\n",
    "                life_buffer.append(life)\n",
    "                pistol_buffer.append(misc[1])\n",
    "                kills_buffer.append(misc[2])\n",
    "                shotgun_buffer.append(misc[3])\n",
    "                minigun_buffer.append(misc[4])\n",
    "                plasma_buffer.append(misc[5])\n",
    "                rocket_buffer.append(misc[6])\n",
    "                secret_buffer.append(misc[7])\n",
    "                hit_buffer.append(misc[8])\n",
    "                damtaken_buffer.append(misc[9])\n",
    "                item_buffer.append(misc[10]) \n",
    "                armor_buffer.append(misc[11])                \n",
    "                print (\"Episode Finish \", misc)\n",
    "                game.new_episode()\n",
    "                game_state = game.get_state()\n",
    "                misc = game_state.game_variables\n",
    "                x_t1 = game_state.screen_buffer\n",
    "\n",
    "            x_t1 = game_state.screen_buffer\n",
    "            misc = game_state.game_variables\n",
    "\n",
    "            x_t1 = preprocessImg(x_t1, size=(img_rows, img_cols))\n",
    "            x_t1 = np.reshape(x_t1, (1, img_rows, img_cols, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "            r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            if (is_terminated):\n",
    "                life = 0\n",
    "            else:\n",
    "                life += 1\n",
    "\n",
    "        #update the cache\n",
    "            prev_misc = misc\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory and decrease epsilon\n",
    "            agent.replay_memory(s_t, action_idx, r_t, s_t1, is_terminated, t)\n",
    "\n",
    "        # Do the training\n",
    "            if t > agent.observe and t % agent.timestep_per_train == 0:\n",
    "                loss = agent.train_replay()\n",
    "\n",
    "            s_t = s_t1\n",
    "            t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                print(\"Now we save model\")\n",
    "                agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/c51_ddqn.h5\")\n",
    "                agent.model.save_weights(\"/home/spillingvoid/Downloads/programs/Doom/models/c51_ddqn.h5\",\n",
    "                                         overwrite=True)\n",
    "\n",
    "        # print info\n",
    "            state = \"\"\n",
    "            if t <= agent.observe:\n",
    "                state = \"observe\"\n",
    "            elif t > agent.observe and t <= agent.observe + agent.explore:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "\n",
    "            if (is_terminated):\n",
    "                print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \\\n",
    "                      \"/ EPSILON\", agent.epsilon, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \\\n",
    "                      \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "            # Save Agent's Performance Statistics\n",
    "                if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                    print(\"Update Rolling Statistics\")\n",
    "                    agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                    agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                    agent.mavg_pistol_left.append(np.mean(np.array(pistol_buffer)))\n",
    "                    agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "                    agent.mavg_shotgun_left.append(np.mean(np.array(shotgun_buffer)))\n",
    "                    agent.mavg_minigun_left.append(np.mean(np.array(minigun_buffer)))\n",
    "                    agent.mavg_plasma_left.append(np.mean(np.array(plasma_buffer)))\n",
    "                    agent.mavg_rocket_left.append(np.mean(np.array(rocket_buffer)))\n",
    "                    agent.mavg_secret_left.append(np.mean(np.array(secret_buffer)))\n",
    "                    agent.mavg_hit_count.append(np.mean(np.array(hit_buffer)))\n",
    "                    agent.mavg_damage_given.append(np.mean(np.array(damtaken_buffer)))\n",
    "                    agent.mavg_item_collected.append(np.mean(np.array(item_buffer)))\n",
    "                    agent.mavg_armor.append(np.mean(np.array(armor_buffer)))\n",
    "                # Reset rolling stats buffer\n",
    "                    life_buffer, pistol_buffer, kills_buffer, shotgun_buffer, minigun_buffer, plasma_buffer, rocket_buffer, secret_buffer, hit_buffer, damtaken_buffer, item_buffer, armor_buffer = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "                # Write Rolling Statistics to file\n",
    "                    with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/c51_ddqn_stats.txt\", \"a+\") as stats_file:\n",
    "                        stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                        stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                        stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                        stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                        stats_file.write('mavg_pistol_left: ' + str(agent.mavg_pistol_left) + '\\n')\n",
    "                        stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')\n",
    "                        stats_file.write('mavg_shotgun_left: ' + str(agent.mavg_shotgun_left) + '\\n')\n",
    "                        stats_file.write('mavg_minigun_left: ' + str(agent.mavg_minigun_left) + '\\n')\n",
    "                        stats_file.write('mavg_plasma_left: ' + str(agent.mavg_plasma_left) + '\\n')\n",
    "                        stats_file.write('mavg_rocket_left: ' + str(agent.mavg_rocket_left) + '\\n')\n",
    "                        stats_file.write('mavg_secret_left: ' + str(agent.mavg_secret_left) + '\\n')\n",
    "                        stats_file.write('mavg_hit_count: ' + str(agent.mavg_hit_count) + '\\n')\n",
    "                        stats_file.write('mavg_damage_given: ' + str(agent.mavg_damage_given) + '\\n')\n",
    "                        stats_file.write('mavg_item_collected: ' + str(agent.mavg_item_collected) + '\\n')\n",
    "                        stats_file.write('mavg_armor: ' + str(agent.mavg_armor) + '\\n')\n",
    "                \n",
    "                if GAME >= agent.maxGAME:\n",
    "                    game.close()\n",
    "                    print(\"======================================\")\n",
    "                    print(\"Training is finished.\")\n",
    "                    agent.model.save(\"/home/spillingvoid/Downloads/programs/Doom/models/dueling_ddqn.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete\")\n",
    "endtime = time()\n",
    "print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
