{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spillingvoid/anaconda3/lib/python3.8/site-packages/skimage/viewer/utils/__init__.py:1: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  from .core import *\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.viewer import ImageViewer\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import backend as K\n",
    "import vizdoom as vzd\n",
    "from vizdoom import DoomGame, ScreenResolution\n",
    "from vizdoom import *\n",
    "import itertools as it\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "\n",
    "#from networks import Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 1:\n",
    "    print(\"GPU available\")\n",
    "    DEVICE = \"/gpu:0\"\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImg(img, size):\n",
    "\n",
    "    img = np.rollaxis(img, 0, 2)    # It becomes (640, 480, 3)\n",
    "    img = skimage.transform.resize(img, size)\n",
    "    img = skimage.color.rgb2gray(img) \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "teststart = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def policy_reinforce(input_shape, action_size, learning_rate):\n",
    "    \"\"\"\n",
    "    Model for REINFORCE\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, 8, strides=(4,4), input_shape=(input_shape)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(64, 4, strides=(2,2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(64, 3, 3, padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(64))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.Dense(action_size, activation='softmax'))\n",
    "\n",
    "    adam = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=adam)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device(DEVICE):\n",
    "class REINFORCEAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.observe = 0\n",
    "        self.frame_per_action = 4 # Frame skipping\n",
    "\n",
    "        # These are hyper parameters for the Policy Gradient\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "\n",
    "        # Model for policy network\n",
    "        self.model = None\n",
    "\n",
    "        # Store episode states, actions and rewards\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        # Performance Statistics\n",
    "        self.stats_window_size= 50 # window size for computing rolling statistics\n",
    "        self.mavg_score = [] # Moving Average of Survival Time\n",
    "        self.var_score = [] # Variance of Survival Time\n",
    "        self.mavg_ammo_left = [] # Moving Average of Ammo used\n",
    "        self.mavg_kill_counts = [] # Moving Average of Kill Counts\n",
    "\n",
    "    # Use the output of policy network, pick action stochastically (Stochastic Policy)\n",
    "    def get_action(self, state):\n",
    "        policy = self.model.predict(state).flatten()\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0], policy\n",
    "\n",
    "    # Instead agent uses sample returns for evaluating policy\n",
    "    # Use TD(1) i.e. Monte Carlo updates \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            if rewards[t] != 0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    # save <s, a ,r> of each step\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self):\n",
    "        episode_length = len(self.states)\n",
    "\n",
    "        discounted_rewards = self.discount_rewards(self.rewards)\n",
    "        # Standardized discounted rewards\n",
    "        discounted_rewards -= np.mean(discounted_rewards) \n",
    "        if np.std(discounted_rewards):\n",
    "            discounted_rewards /= np.std(discounted_rewards)\n",
    "        else:\n",
    "            self.states, self.actions, self.rewards = [], [], []\n",
    "            print ('std = 0!')\n",
    "            return 0\n",
    "\n",
    "        update_inputs = np.zeros(((episode_length,) + self.state_size)) # Episode_lengthx64x64x4\n",
    "        # Similar to one-hot target but the \"1\" is replaced by discounted_rewards R_t\n",
    "        advantages = np.zeros((episode_length, self.action_size))\n",
    "\n",
    "        # Episode length is like the minibatch size in DQN\n",
    "        for i in range(episode_length):\n",
    "            update_inputs[i,:,:,:] = self.states[i]\n",
    "            advantages[i][self.actions[i]] = discounted_rewards[i]\n",
    "        \n",
    "        loss = self.model.fit(update_inputs, advantages, epochs=1, verbose=0)\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        return loss.history['loss']\n",
    "\n",
    "\n",
    "    def shape_reward(self, r_t, misc, prev_misc, t):\n",
    "        \n",
    "        # Check any kill count\n",
    "        if (misc[0] > prev_misc[0]):\n",
    "            r_t = r_t + 310\n",
    "\n",
    "        if (misc[1] < prev_misc[1]): # Use ammo\n",
    "            r_t = r_t - 10\n",
    "\n",
    "        if (misc[2] < prev_misc[2]): # Loss HEALTH\n",
    "            r_t = r_t - 0.1\n",
    "\n",
    "        return r_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 6, 10, 32)         8224      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6, 10, 32)         128       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 6, 10, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 4, 64)          32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 2, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 89,443\n",
      "Trainable params: 88,931\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-18105c2c7f9f>:5: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  img = skimage.color.rgb2gray(img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.   0. 100.] [0.3287314  0.33329135 0.3379772 ]\n",
      "TIME 14 / GAME 0 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 13 / LOSS [-0.189605250954628]\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.   0. 100.] [0.32852456 0.33404124 0.33743423]\n",
      "TIME 89 / GAME 1 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [-0.09674941748380661]\n",
      "Episode Finish  [  0.   0. 100.] [0.3311667  0.33681402 0.33201927]\n",
      "TIME 96 / GAME 2 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [0.14436747133731842]\n",
      "Episode Finish  [  0.   0. 100.] [0.33078173 0.33695245 0.3322658 ]\n",
      "TIME 112 / GAME 3 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [-0.051391854882240295]\n",
      "Episode Finish  [  0.   0. 100.] [0.32951087 0.33862036 0.33186877]\n",
      "TIME 132 / GAME 4 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [-0.16213929653167725]\n",
      "Episode Finish  [  0.   0. 100.] [0.33022603 0.34088925 0.3288847 ]\n",
      "TIME 171 / GAME 5 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [-0.036891452968120575]\n",
      "Episode Finish  [  0.   0. 100.] [0.326885   0.34375426 0.32936075]\n",
      "TIME 200 / GAME 6 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.05875854566693306]\n",
      "Episode Finish  [  0.   0. 100.] [0.32707834 0.34580475 0.32711685]\n",
      "TIME 275 / GAME 7 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [-0.14528846740722656]\n",
      "Episode Finish  [  0.   0. 100.] [0.32572916 0.34903702 0.3252338 ]\n",
      "TIME 295 / GAME 8 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [-0.13999995589256287]\n",
      "Episode Finish  [  0.   0. 100.] [0.3237491  0.35140818 0.32484278]\n",
      "TIME 302 / GAME 9 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.25594082474708557]\n",
      "Episode Finish  [  0.   0. 100.] [0.32213086 0.3503739  0.32749525]\n",
      "TIME 377 / GAME 10 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [0.19707420468330383]\n",
      "Episode Finish  [  0.   0. 100.] [0.32384834 0.35799566 0.318156  ]\n",
      "TIME 387 / GAME 11 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [0.22819137573242188]\n",
      "Episode Finish  [  0.   0. 100.] [0.3229755  0.36150697 0.31551757]\n",
      "TIME 394 / GAME 12 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.13671599328517914]\n",
      "Episode Finish  [  0.   0. 100.] [0.3246851  0.36263132 0.3126836 ]\n",
      "TIME 461 / GAME 13 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.05599541217088699]\n",
      "Episode Finish  [  0.   0. 100.] [0.3177746  0.3709158  0.31130964]\n",
      "TIME 476 / GAME 14 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.19934915006160736]\n",
      "Episode Finish  [  0.   0. 100.] [0.31701964 0.37354442 0.3094359 ]\n",
      "TIME 551 / GAME 15 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [0.046778354793787]\n",
      "Episode Finish  [  0.   0. 100.] [0.3190394  0.37798774 0.30297285]\n",
      "TIME 624 / GAME 16 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [-0.025637978687882423]\n",
      "Episode Finish  [  0.   0. 100.] [0.31344953 0.3874124  0.29913807]\n",
      "TIME 631 / GAME 17 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.3094162344932556]\n",
      "Episode Finish  [  0.   0. 100.] [0.31340832 0.38649642 0.30009526]\n",
      "TIME 706 / GAME 18 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [0.028166212141513824]\n",
      "Episode Finish  [  0.   0. 100.] [0.30987403 0.3968256  0.2933003 ]\n",
      "TIME 781 / GAME 19 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [-0.05282687023282051]\n",
      "Episode Finish  [  0.   0. 100.] [0.30691388 0.40075076 0.29233533]\n",
      "TIME 856 / GAME 20 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [-0.07093855738639832]\n",
      "Episode Finish  [  0.   0. 100.] [0.30717885 0.4067915  0.2860296 ]\n",
      "TIME 873 / GAME 21 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.09074479341506958]\n",
      "Episode Finish  [  0.   0. 100.] [0.30737814 0.40874785 0.28387395]\n",
      "TIME 948 / GAME 22 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [0.03308987244963646]\n",
      "Episode Finish  [  0.   0. 100.] [0.30585685 0.4105797  0.28356344]\n",
      "TIME 1023 / GAME 23 / STATE Train mode / ACTION 1 / REWARD -9.0 / LIFE 74 / LOSS [-0.05529646575450897]\n",
      "Episode Finish  [  0.   0. 100.] [0.30676168 0.41464114 0.2785972 ]\n",
      "TIME 1031 / GAME 24 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.375160276889801]\n",
      "Episode Finish  [  0.   0. 100.] [0.30314904 0.41642243 0.28042862]\n",
      "TIME 1053 / GAME 25 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [-0.22480592131614685]\n",
      "Episode Finish  [  0.   0. 100.] [0.3089867  0.41032532 0.28068796]\n",
      "TIME 1098 / GAME 26 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [0.06753329932689667]\n",
      "Episode Finish  [  0.   0. 100.] [0.30618152 0.42398605 0.2698325 ]\n",
      "TIME 1117 / GAME 27 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.31643620133399963]\n",
      "Episode Finish  [  0.   0. 100.] [0.30371314 0.41660532 0.2796816 ]\n",
      "TIME 1192 / GAME 28 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [0.019020894542336464]\n",
      "Episode Finish  [  0.   0. 100.] [0.30009958 0.4273639  0.2725365 ]\n",
      "TIME 1200 / GAME 29 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.4407608211040497]\n",
      "Episode Finish  [  0.   0. 100.] [0.30411267 0.41741633 0.278471  ]\n",
      "TIME 1228 / GAME 30 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [0.007868043147027493]\n",
      "Episode Finish  [  0.   0. 100.] [0.30474818 0.42936647 0.26588538]\n",
      "TIME 1237 / GAME 31 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.18424484133720398]\n",
      "Episode Finish  [  0.   0. 100.] [0.300522   0.42837706 0.2711009 ]\n",
      "TIME 1250 / GAME 32 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [0.17312270402908325]\n",
      "Episode Finish  [  0.   0. 100.] [0.29861176 0.43069434 0.27069387]\n",
      "TIME 1325 / GAME 33 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [-0.01933477632701397]\n",
      "Episode Finish  [  0.   0. 100.] [0.29982007 0.4339193  0.26626068]\n",
      "TIME 1353 / GAME 34 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.1536397486925125]\n",
      "Episode Finish  [  0.   0. 100.] [0.29276636 0.43084875 0.27638483]\n",
      "TIME 1428 / GAME 35 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [0.048194367438554764]\n",
      "Episode Finish  [  0.   0. 100.] [0.29406443 0.43979478 0.2661408 ]\n",
      "TIME 1441 / GAME 36 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.09658704698085785]\n",
      "Episode Finish  [  0.   0. 100.] [0.29500377 0.441578   0.26341826]\n",
      "TIME 1516 / GAME 37 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [0.09566039592027664]\n",
      "Episode Finish  [  0.   0. 100.] [0.29775858 0.44436523 0.2578761 ]\n",
      "TIME 1591 / GAME 38 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [-0.05213220417499542]\n",
      "Episode Finish  [  0.   0. 100.] [0.29575622 0.45055443 0.25368932]\n",
      "TIME 1666 / GAME 39 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [0.09494034945964813]\n",
      "Episode Finish  [  0.   0. 100.] [0.29423377 0.4507554  0.25501087]\n",
      "TIME 1741 / GAME 40 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [-0.08929891139268875]\n",
      "Episode Finish  [  0.   0. 100.] [0.29744455 0.45371172 0.24884373]\n",
      "TIME 1755 / GAME 41 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.10235381126403809]\n",
      "Episode Finish  [  0.   0. 100.] [0.2977105  0.45169672 0.25059274]\n",
      "TIME 1830 / GAME 42 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [-0.0007012367132119834]\n",
      "Episode Finish  [  0.   0. 100.] [0.29363775 0.4508929  0.25546932]\n",
      "TIME 1866 / GAME 43 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.44148677587509155]\n",
      "Episode Finish  [  0.   0. 100.] [0.29840723 0.45258418 0.24900866]\n",
      "TIME 1889 / GAME 44 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [-0.0723104476928711]\n",
      "Episode Finish  [  0.   0. 100.] [0.2915139  0.45557064 0.25291547]\n",
      "TIME 1964 / GAME 45 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [-0.15288156270980835]\n",
      "Episode Finish  [  0.   0. 100.] [0.29123157 0.4607902  0.24797823]\n",
      "TIME 2039 / GAME 46 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [0.0011802546214312315]\n",
      "Episode Finish  [  0.   0. 100.] [0.29152316 0.45491913 0.2535577 ]\n",
      "TIME 2086 / GAME 47 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.05069533735513687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Finish  [  0.   0. 100.] [0.29384288 0.45009372 0.25606337]\n",
      "TIME 2108 / GAME 48 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.23301167786121368]\n",
      "Episode Finish  [  0.   0. 100.] [0.29155388 0.4525687  0.25587744]\n",
      "TIME 2183 / GAME 49 / STATE Train mode / ACTION 1 / REWARD -4.0 / LIFE 74 / LOSS [-0.023385921493172646]\n",
      "Episode Finish  [  0.   0. 100.] [0.282413   0.46021935 0.2573676 ]\n",
      "TIME 2258 / GAME 50 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [0.07450396567583084]\n",
      "Update Rolling Statistics\n",
      "Episode Finish  [  0.   0. 100.] [0.30034095 0.44817054 0.25148848]\n",
      "TIME 2296 / GAME 51 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.07432994246482849]\n",
      "Episode Finish  [  0.   0. 100.] [0.29072297 0.45411658 0.25516042]\n",
      "TIME 2371 / GAME 52 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [0.07315116375684738]\n",
      "Episode Finish  [  0.   0. 100.] [0.28001198 0.4652858  0.25470227]\n",
      "TIME 2446 / GAME 53 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [0.010471852496266365]\n",
      "Episode Finish  [  0.   0. 100.] [0.2925515  0.45444617 0.25300235]\n",
      "TIME 2512 / GAME 54 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [0.14462924003601074]\n",
      "Episode Finish  [  0.   0. 100.] [0.30258816 0.45497563 0.24243619]\n",
      "TIME 2554 / GAME 55 / STATE Train mode / ACTION 1 / REWARD 103.0 / LIFE 74 / LOSS [-0.0804254338145256]\n",
      "Episode Finish  [  0.   0. 100.] [0.30021325 0.45568508 0.2441017 ]\n",
      "TIME 2565 / GAME 56 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [-0.11089996993541718]\n",
      "Episode Finish  [  0.   0. 100.] [0.2754943  0.46068    0.26382565]\n",
      "TIME 2640 / GAME 57 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [0.07103527337312698]\n",
      "Episode Finish  [  0.   0. 100.] [0.29175764 0.4559017  0.25234067]\n",
      "TIME 2655 / GAME 58 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.3703749477863312]\n",
      "Episode Finish  [  0.   0. 100.] [0.2629704  0.4605096  0.27651995]\n",
      "TIME 2730 / GAME 59 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [0.0009758250089362264]\n",
      "Episode Finish  [  0.   0. 100.] [0.27358672 0.45919615 0.2672172 ]\n",
      "TIME 2805 / GAME 60 / STATE Train mode / ACTION 0 / REWARD -4.0 / LIFE 74 / LOSS [0.10346131026744843]\n",
      "Episode Finish  [  0.   0. 100.] [0.29665568 0.4438329  0.2595114 ]\n",
      "TIME 2880 / GAME 61 / STATE Train mode / ACTION 2 / REWARD -4.0 / LIFE 74 / LOSS [0.03013259917497635]\n",
      "Episode Finish  [  0.   0. 100.] [0.28660524 0.4591856  0.25420916]\n",
      "TIME 2919 / GAME 62 / STATE Train mode / ACTION 0 / REWARD 103.0 / LIFE 74 / LOSS [0.10309123992919922]\n",
      "Episode Finish  [  0.   0. 100.] [0.28875104 0.44840047 0.26284847]\n",
      "TIME 2953 / GAME 63 / STATE Train mode / ACTION 2 / REWARD 103.0 / LIFE 74 / LOSS [-0.15859977900981903]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Avoid Tensorflow eats up GPU memory\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    #K.set_session(sess)\n",
    "    with tf.device(DEVICE):\n",
    "        game = vzd.DoomGame()\n",
    "        game.load_config(\"/home/spillingvoid/Downloads/programs/ViZDoom/scenarios/rocket_basic.cfg\")\n",
    "        game.add_available_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "        game.add_available_game_variable(vzd.GameVariable.AMMO2)\n",
    "        game.add_available_game_variable(vzd.GameVariable.HEALTH)\n",
    "        game.set_window_visible(True)\n",
    "        game.set_mode(vzd.Mode.PLAYER)\n",
    "        game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "        game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "        game.get_available_buttons_size()\n",
    "        game.init()\n",
    "\n",
    "    # Maximum number of episodes\n",
    "        max_episodes = 64 #128\n",
    "    \n",
    "        game.new_episode()\n",
    "        game_state = game.get_state()\n",
    "        misc = game_state.game_variables  # [KILLCOUNT, AMMO, HEALTH]\n",
    "        prev_misc = misc\n",
    "\n",
    "        action_size = game.get_available_buttons_size()\n",
    "\n",
    "        img_rows , img_cols = 30, 45\n",
    "    # Convert image into Black and white\n",
    "        img_channels = 4 # We stack 4 frames\n",
    "\n",
    "        state_size = (img_rows, img_cols, img_channels)\n",
    "        agent = REINFORCEAgent(state_size, action_size)\n",
    "\n",
    "        agent.model = policy_reinforce(state_size, action_size, agent.learning_rate)\n",
    "\n",
    "    # Start training\n",
    "        GAME = 0\n",
    "        t = 0\n",
    "        max_life = 0 # Maximum episode life (Proxy for agent performance)\n",
    "\n",
    "    # Buffer to compute rolling statistics \n",
    "        life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "        for i in range(max_episodes):\n",
    "\n",
    "            game.new_episode()\n",
    "            game_state = game.get_state()\n",
    "            misc = game_state.game_variables \n",
    "            prev_misc = misc\n",
    "\n",
    "            x_t = game_state.screen_buffer # 480 x 640\n",
    "            x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "            s_t = np.stack(([x_t]*4), axis=2) # It becomes 64x64x4\n",
    "            s_t = np.expand_dims(s_t, axis=0) # 1x64x64x4\n",
    "\n",
    "            life = 0 # Episode life\n",
    "\n",
    "            while not game.is_episode_finished():\n",
    "\n",
    "                loss = 0 # Training Loss at each update\n",
    "                r_t = 0 # Initialize reward at time t\n",
    "                a_t = np.zeros([action_size]) # Initialize action at time t\n",
    "\n",
    "                x_t = game_state.screen_buffer\n",
    "                x_t = preprocessImg(x_t, size=(img_rows, img_cols))\n",
    "                x_t = np.reshape(x_t, (1, img_rows, img_cols, 1))\n",
    "                s_t = np.append(x_t, s_t[:, :, :, :3], axis=3)\n",
    "                \n",
    "            # Sample action from stochastic softmax policy\n",
    "                action_idx, policy  = agent.get_action(s_t)\n",
    "                a_t[action_idx] = 1 \n",
    "\n",
    "                a_t = a_t.astype(int)\n",
    "                game.set_action(a_t.tolist())\n",
    "                skiprate = agent.frame_per_action # Frame Skipping = 4\n",
    "                game.advance_action(skiprate)\n",
    "\n",
    "                r_t = game.get_last_reward()  # Each frame we get reward of 0.1, so 4 frames will be 0.4\n",
    "            # Check if episode is terminated\n",
    "                is_terminated = game.is_episode_finished()\n",
    "\n",
    "                if (is_terminated):\n",
    "                # Save max_life\n",
    "                    if (life > max_life):\n",
    "                        max_life = life \n",
    "                    life_buffer.append(life)\n",
    "                    ammo_buffer.append(misc[1])\n",
    "                    kills_buffer.append(misc[0])\n",
    "                    print (\"Episode Finish \", prev_misc, policy)\n",
    "                else:\n",
    "                    life += 1\n",
    "                    game_state = game.get_state()  # Observe again after we take the action\n",
    "                    misc = game_state.game_variables\n",
    "\n",
    "            # Reward Shaping\n",
    "                r_t = agent.shape_reward(r_t, misc, prev_misc, t)\n",
    "\n",
    "            # Save trajactory sample <s, a, r> to the memory\n",
    "                agent.append_sample(s_t, action_idx, r_t)\n",
    "\n",
    "            # Update the cache\n",
    "                t += 1\n",
    "                prev_misc = misc\n",
    "\n",
    "                if (is_terminated and t > agent.observe):\n",
    "                # Every episode, agent learns from sample returns\n",
    "                    loss = agent.train_model()\n",
    "\n",
    "            # Save model every 10000 iterations\n",
    "                if t % 10000 == 0:\n",
    "                    print(\"Save model\")\n",
    "                    agent.model.save_weights(\"models/reinforce.h5\", overwrite=True)\n",
    "\n",
    "                state = \"\"\n",
    "                if t <= agent.observe:\n",
    "                    state = \"Observe mode\"\n",
    "                else:\n",
    "                    state = \"Train mode\"\n",
    "\n",
    "                if (is_terminated):\n",
    "\n",
    "                    # Print performance statistics at every episode end\n",
    "                    print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \"/ ACTION\", action_idx, \"/ REWARD\", r_t, \"/ LIFE\", max_life, \"/ LOSS\", loss)\n",
    "\n",
    "                # Save Agent's Performance Statistics\n",
    "                    if GAME % agent.stats_window_size == 0 and t > agent.observe: \n",
    "                        print(\"Update Rolling Statistics\")\n",
    "                        agent.mavg_score.append(np.mean(np.array(life_buffer)))\n",
    "                        agent.var_score.append(np.var(np.array(life_buffer)))\n",
    "                        agent.mavg_ammo_left.append(np.mean(np.array(ammo_buffer)))\n",
    "                        agent.mavg_kill_counts.append(np.mean(np.array(kills_buffer)))\n",
    "\n",
    "                    # Reset rolling stats buffer\n",
    "                        life_buffer, ammo_buffer, kills_buffer = [], [], [] \n",
    "\n",
    "                    # Write Rolling Statistics to file\n",
    "                        with open(\"/home/spillingvoid/Downloads/programs/Doom/statistics/reinforce_stats.txt\", \"a+\") as stats_file:\n",
    "                            stats_file.write('Game: ' + str(GAME) + '\\n')\n",
    "                            stats_file.write('Max Score: ' + str(max_life) + '\\n')\n",
    "                            stats_file.write('mavg_score: ' + str(agent.mavg_score) + '\\n')\n",
    "                            stats_file.write('var_score: ' + str(agent.var_score) + '\\n')\n",
    "                            stats_file.write('mavg_ammo_left: ' + str(agent.mavg_ammo_left) + '\\n')\n",
    "                            stats_file.write('mavg_kill_counts: ' + str(agent.mavg_kill_counts) + '\\n')\n",
    "        \n",
    "        # Episode Finish. Increment game count\n",
    "            GAME += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n",
      " Test Time elapsed: 12.45 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Training complete\")\n",
    "endtime = time()\n",
    "print(\" Test Time elapsed: %.2f minutes\" % ((endtime - teststart) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
